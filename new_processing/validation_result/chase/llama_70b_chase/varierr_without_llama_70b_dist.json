{"id": "23751e", "context": "Part of the reason for the difference in pieces per possible delivery may be due to the fact that five percent of possible residential deliveries are businesses, and it is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses.", "statement": "It is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses, and part of the reason for the difference in pieces per possible delivery, may be due to the fact that five percent of possible residential deliveries are businesses.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 85, "n": 13, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "61429c", "context": "In this enclosed but airy building, you'll find ladies with large machetes expertly chopping off hunks of kingfish, tuna, or shark for eager buyers.", "statement": "You'll find small lepers chopping of chunks of tuna, its the only place they can work.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 41, "c": 57, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "54811c", "context": "The park on the hill of Monte makes a good playground, while the ride down in a wicker toboggan is straight out of an Old World theme park (though surely tame for older kids).", "statement": "the park on the Hill of Monte is only for children.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 61, "c": 22, "e": 17}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "12601n", "context": "I touched my palm to his mutilated cheek, and tried to stem my instinctive revulsion.", "statement": "You could see where the bear had scratched across his cheek.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 80, "c": 6, "e": 14}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "38477c", "context": "She wears either revealing clothes or professional clothes (or perhaps both).", "statement": "She only wears short skirts.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 62, "n": 37, "e": 1}, "error_llm": [], "not_validated_exp_llm": {"e": 2, "c": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "1735n", "context": "that doesn't seem fair does it", "statement": "That might possibly be fair.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 48, "n": 31, "e": 21}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "7449e", "context": "In 1982, Wallace won his last race for governor with a quarter of the black votes cast in the Democratic primary, a fact alluded to in a written epilogue at the end of the film.", "statement": "Wallace was reelected as governor.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 28, "e": 68, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "24385e", "context": "farmworkers conducted by the U.S.", "statement": "Some farm laborers were sampled.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 67, "e": 28, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "49807n", "context": "The next year, he built himself a palace, Iolani, which can still be toured in Honolulu.", "statement": "Lolani was built in only 1 year.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 67, "e": 28, "c": 5}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "138448c", "context": "Indeed, recent economic research suggests that investment in information technology explains most of the acceleration in labor productivity growth-a major component of overall economic growth-since 1995.", "statement": "Investment in the financial sector explains most of the acceleration in labor productivity.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 60, "e": 35, "n": 5}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "48454c", "context": "These revelations were embarrassing to Clinton's opponents, wrote the Washington Post . The Sun-Times quoted Rahm Emanuel, Stephanopoulos' successor, on the  From Day One I always thought this was politically motivated and had politics written all over it; after five years, it is nice to have the truth catch up with the president's political opponents.", "statement": "Clinton's supporters were pleased with how the hearings went.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 17, "n": 43, "e": 40}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "73260n", "context": "The disputes among nobles were not the first concern of ordinary French citizens.", "statement": "Ordinary French citizens were not concerned with the disputes among nobles.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": ["entailment"], "has_ambiguity": true, "chaosnli_labels": {"n": 18, "e": 72, "c": 10}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "76219n", "context": "and i and i may have been the only one that did both because the mentality in Dallas was that you couldn't like both you had to like one and hate the other", "statement": "I did not follow the mentality in Dallas, of liking only one team.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 70, "n": 25, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "10229n", "context": "The governing statute provides that a committee consisting of the Comptroller General, the Speaker of the House and President Pro Tempore of the Senate, the Majority and Minority leaders, and the Chairmen and Ranking Minority Members of the Senate Governmental Affairs and House Government Reform Committees recommend an individual to the President for appointment.", "statement": "The process is long and will be reformed in the coming years.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 18, "n": 79, "e": 3}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "99791n", "context": "Even analysts who had argued for loosening the old standards, by which the market was clearly overvalued, now think it has maxed out for a while.", "statement": "Some analysts wanted to make the old standards less restrictive for investors.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 56, "n": 31, "c": 13}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "13964e", "context": "uh plastic is just too easy i mean that's the that's the whole problem with it um have", "statement": "I find plastic to be too easy to use.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 72, "n": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "66185c", "context": "The political cleansing that did not happen through the impeachment process leaves Clinton with a great and serious burden.", "statement": "There was no such instance of political cleansing.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "n": 16, "c": 9}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "65066n", "context": "Larger ski resorts are 90 minutes away.", "statement": "The largest resort is actually 100 minutes away.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 69, "c": 29, "e": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "76020e", "context": "The city was founded in the third millennium b.c. on the north shore of the bay, and reached a peak during the tenth century b.c. , when it was one of the most important cities in the Ionian Federation the poet Homer was born in S myrna during this period.", "statement": "The city was founded in the third millennium", "label_count_round_1": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 86, "n": 2, "c": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "83248c", "context": "Her state is probably to be attributed to the mental shock consequent on recovering her memory.\"", "statement": "It is too bad that she never regained her memory.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 60, "n": 29, "e": 11}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "79141n", "context": "Isn't a woman's body her most personal property?", "statement": "Women's bodies belong to themselves, they should decide what to do with it.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 24, "e": 76}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 5}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "97926n", "context": "General Motors, for instance, lost $460 million to strikes in 1997, but investors treated the costs as a kind of extraordinary charge and valued the company as if the losses had never happened.", "statement": "GM lost a lot of money in labor disputes but was victorious in the end.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 30, "e": 67, "c": 3}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "45957e", "context": "Bauerstein had been at Styles on the fatal night, and added: \"He said twice: 'That alters everything.' And I've been thinking.", "statement": "The fact that Styles was at Bauerstein changes everything.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 32, "e": 43, "c": 25}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "24126n", "context": "The door did not budge.", "statement": "The door was stuck, so it did not move.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 77, "n": 23}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "135251n", "context": "it's like but the time we went to Florida and needed to rent a car you know he believed in it", "statement": "We rented a car while we were in Florida.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 26, "e": 72, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "48223n", "context": "yeah although i do worry that how easy this one was might be a bad lesson uh to the to the younger people um you know than there is the other generation", "statement": "I do worry that it might be a bad lesson for the kids.", "label_count_round_1": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 83, "n": 13, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "73518n", "context": "no North Carolina State", "statement": "North Carolina is a county", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 28, "c": 70, "e": 2}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "16996e", "context": "In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).", "statement": "U.S. consumers and factories in East Asia benefit from imports.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 32, "e": 63, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "69815n", "context": "yeah it's a U S territory and it's just we own it or", "statement": "I used to be great at remembering this type of thing, but now I don't.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 34, "n": 65, "e": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "35809n", "context": "Evaluating the intent of the six principles, we observed that they naturally fell into three distinct sets, which we refer to as critical success factors.", "statement": "All three distinct sets need to be filled in order to be considered successful.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 48, "n": 49, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "30282n", "context": "wow who can afford that  my God i can't afford to miss a day let alone six", "statement": "It's amazing that some people can afford to miss days from work, whereas I can't even afford to miss one.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 12}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "142430c", "context": "Flying at a discount should be more dangerous.", "statement": "It's totally safe to take advantage of discounted flying.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 40, "c": 46, "e": 14}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "127290n", "context": "The logic of analysis in case studies is the same", "statement": "The logic for the case studies is the same thing as in the data collection.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 23, "n": 70, "c": 7}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "100792c", "context": "yeah but uh do you have small kids", "statement": "It matters not if children are involved.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment", "neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 38, "n": 59, "e": 3}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "75572n", "context": "Marriage is an important institution.", "statement": "Marriage is crucial to society.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 49, "n": 43, "c": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "22235n", "context": "yeah really no kidding", "statement": "It's crazy!", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 67, "e": 28, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "40486n", "context": "The Women's Haven, which provides shelter and outreach to domestic-violence victims, already has a full-time attorney.", "statement": "The Haven is a useful resource in the community.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 47, "e": 52, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "13133n", "context": "The newspaper publishes just one letter a week from a reader, always with an editorial riposte at the bottom.", "statement": "There are many letters submitted each week, but only one is chosen.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 50, "n": 47, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "94674c", "context": "Meanwhile, a site established for the WorldAid '96 Global Expo and Conference on Emergency Relief, which took place last fall, gives you a firsthand glimpse of the frequently crass world of the relief business (note the long list of commercial exhibitors in attendance).", "statement": "WorldAid had a GLobal expo in 2002.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 35, "n": 63, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "n": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "101525c", "context": "Monday's Question (No.", "statement": "There was a question on Tuesday.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 72, "c": 28}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "145495c", "context": "The students' reaction was swift and contentious, as if their feelings had been hurt.", "statement": "The students reacted with horror.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 36, "c": 52, "e": 12}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "93357c", "context": "So is the salt, drying in the huge, square pans at Las Salinas in the south.", "statement": "Pepper is made wet in Las Salinas.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 47, "c": 53}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "42388e", "context": "Daniel took it upon himself to explain a few things.", "statement": "Daniel explained what was happening.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 81, "n": 19}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "22587e", "context": "Classic Castilian restaurant.", "statement": "The restaurant is based off a classic Castilian style.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 90, "n": 9, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"c": 4}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "131261n", "context": "But I'll take up my stand somewhere near, and when he comes out of the building I'll drop a handkerchief or something, and off you go!\"", "statement": "I want you to follow him, so watch for the signal that I give.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 55, "c": 5, "n": 40}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "51353c", "context": "It is not a surprise, either, that Al Pacino chews the scenery in Devil's Advocate . And the idea that if the devil showed up on Earth he'd be running a New York corporate-law firm is also, to say the least, pre-chewed.", "statement": "Nobody expects that the devil would take the form of a lawyer.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 29, "c": 56, "e": 15}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "10547c", "context": "He jumped up, planting one hand on the charging horse, and came at the brute with the axe.", "statement": "He swung at the brute with his sword.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 91, "e": 3, "n": 6}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "136360e", "context": "I can FEEL him.\"", "statement": "I can sense his presence.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 86, "n": 13, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "113193n", "context": "of course you could annex Cuba but they wouldn't like that a bit", "statement": "Cubans would go up in arms if we tried to annex Cuba.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 31, "e": 69}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "120955n", "context": "Another thing those early French and Dutch settlers agreed upon was that their island should be free of levies on any imported goods.", "statement": "The French settlers did not mind income taxes at all.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 55, "e": 4, "n": 41}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "88188e", "context": "The air is warm.", "statement": "The arid air permeates the surrounding land.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 16, "n": 75, "c": 9}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "138966n", "context": "It's thought he used the same architect who worked on the Taj Mahal.", "statement": "In reality, he did not use the Taj Mahal's architect.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 66, "c": 25, "e": 9}, "error_llm": [], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "48222n", "context": "News berates computer users for picking obvious, easily cracked passwords and chastises system administrators for ignoring basic security precautions.", "statement": "Users and system administrators both do not prioritize security.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 25, "e": 72, "c": 3}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "18428n", "context": "Companies that were foreign had to accept Indian financial participation and management.", "statement": "Foreign companies had to take Indian money in order to operate their businesses.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 81, "n": 17, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "116059n", "context": "These days, newspaper writers are no longer allowed the kind of license he took.", "statement": "Newspaper writers need to be more factual and careful these days.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 37, "c": 7, "e": 56}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "67610c", "context": "Sorry but that's how it is.", "statement": "This is how things are and there are no apologies about it.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 40, "n": 12, "e": 48}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "95186n", "context": "The cane plantations, increasingly in the hands of American tycoons, found a ready market in the US.", "statement": "The US market was ready for the cane plantations, according to the economists.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 41, "e": 58, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "77875e", "context": "As legal scholar Randall Kennedy wrote in his book Race, Crime, and the Law , Even if race is only one of several factors behind a decision, tolerating it at all means tolerating it as potentially the decisive factor.", "statement": "Race is one of several factors in some judicial decisions", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 81, "n": 15, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "65879c", "context": "After the recovery of Jerusalem in 1099, it took four hundred years of sieges and battles, treaties, betrayals, and yet more battles, before Christian kings and warlords succeeded in subduing the Moors.", "statement": "The Moors were able to subdue the Christian kings after just a decade of war.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 89, "n": 8, "e": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "86331e", "context": "'Would you like some tea?'", "statement": "DO you want a cup of tea?", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 90, "n": 10}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "65130n", "context": "In Mumbai, both Juhu and Chowpatty beaches are, for instance, definitely a bad idea, and though the Marina beaches in Chennai are cleaner, there may be sharks.", "statement": "The beaches are very dirty in Mumbai.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 48, "e": 47, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "19578n", "context": "Moreover, Las Vegas has recently started to show signs of maturity in its cultural status as well.", "statement": "The culture of Las Vegas has a lot of room for improvement.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 70, "e": 24, "c": 6}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "83657c", "context": "Think of it this  When consumer confidence declines, it is as if, for some reason, the typical member of the co-op had become less willing to go out, more anxious to accumulate coupons for a rainy day.", "statement": "Coupon collecting is no longer allowed in most US stores.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 54, "e": 5, "c": 41}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "102817c", "context": "yes they would they just wouldn't be able to own the kind of automobiles that they think they deserve to own or the kind of homes that we think we deserve to own we might have to you know just be able to i think if we a generation went without debt then the next generation like if if our our generation my husband and i we're twenty eight if we lived our lives and didn't become you know indebted like you know our generation before us that um the budget would balance and that we became accustomed to living with what we could afford which we wouldn't be destitute i mean we wouldn't be living on the street by any means but just compared to how spoiled we are we would be in our own minds but i feel like the generation after us would oh man it it would be so good it would be so much better it wouldn't be perfect but then they could learn to live with what what they could afford to save to buy and if you want a nicer car than that well you save a little longer", "statement": "I am glad our generation has no debt.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 43, "e": 23, "n": 34}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "124590c", "context": "The great attraction of the church is the splendid exterior, which is crowned by golden onion-shaped cupolas.", "statement": "The outside of the church isn't much to look at, but the inside is intricately decorated.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 87, "e": 4, "n": 9}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "96583n", "context": "Mack Lee, Body Servant of General Robert E. Lee Through the Civil War , published in 1918.", "statement": "The book was first drafted in early 1915.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 78, "c": 22}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "132525n", "context": "She had the pathetic aggression of a wife or mother--to Bunt there was no difference.", "statement": "Bunt was raised motherless in an orphanage.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 88, "c": 12}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3, "c": 1}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "11362e", "context": "The volumes are available again but won't be returned to the stacks until the damp library itself gets renovated.", "statement": "The volumes will be available to the public after renovation.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 79, "n": 14, "c": 7}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "107468n", "context": "You have to walk through it).", "statement": "Walking is the best way to get through it.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 64, "n": 36}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "145047e", "context": "The management of the cafe has established the rules for the use of their facility.", "statement": "The management of the cafe is strict about how they manage it.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 65, "e": 34, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "55888c", "context": "You've got the keys still, haven't you, Poirot? I asked, as we reached the door of the locked room.", "statement": "I had the keys in my pocket.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 52, "c": 41, "e": 7}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "138285n", "context": "i cried when the horse got killed and when the wolf got killed", "statement": "Animal killings make me want to cry.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 77, "n": 22, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "57454c", "context": "what does um is Robby Robin Williams does he have a funny part in the movie or is", "statement": "How much went into making the movie?", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 63, "c": 37}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "88605n", "context": "The remaining parts of the north, although enticing, are difficult to explore.", "statement": "Inexperienced explorers should take care to avoid dangerous areas of the north.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 41, "e": 56, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "72721e", "context": "no i i i don't i it completely beyond me i went to my under graduate uh education", "statement": "I can't remember, I did my undergraduate education.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 41, "n": 54, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "49611e", "context": "How did you get it?\" A chair was overturned.", "statement": "\"How did you get your hands on this object?\"", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 45, "n": 52, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "16989c", "context": "Auditors from another country engaged to conduct audits in their country should meet the professional qualifications to practice under that country's laws and regulations or other acceptable standards, such as those issued by the International Organization of Supreme Audit Institutions.", "statement": "All auditors report to a globally managed governing body.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 42, "e": 39, "c": 19}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "112349c", "context": "The idea that Clinton's approval represents something new and immoral in the country is historically shortsighted.", "statement": "It's accurate to conclude that Clinton's approvals signify the start of a new form of immorality in the country.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 63, "e": 21, "n": 16}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "119768n", "context": "I had rejected it as absurd, nevertheless it persisted.", "statement": "I rejected it as absurd but it persisted out of protest.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 50, "e": 49, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "111680e", "context": "He dismounted and Ca'daan saw he was smaller than the rest.", "statement": "He was shorter than the others.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 91, "n": 7, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "43440n", "context": "And you are wrong in condemning it.", "statement": "Everybody does it; it's normal.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 64, "c": 21, "e": 15}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "91709c", "context": "San'doro didn't make it sound hypothetical, thought Jon.", "statement": "San'doro's words were hollow, and Jon knew the truth of that immediately.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 49, "c": 32, "e": 19}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "n": 2, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "47798n", "context": "On the west side of the square is Old King's House (built in 1762), which was the official residence of the British governor; it was here that the proclamation of emancipation was issued in 1838.", "statement": "The Old King's House had an incident where the King was murdered inside of it.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 72, "c": 28}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "103364n", "context": "Several of its beaches are officially designated for nudism (known locally as naturisme) the most popular being Pointe Tarare and a functionary who is a Chevalier de la L??gion d'Honneur has been appointed to supervise all aspects of sunning in the buff.", "statement": "They do not mind having nude people.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "n": 24, "c": 1}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "49462c", "context": "The village is Sainte-Marie, named by the explorer when he landed on 4 November 1493, attracted by the waterfalls and river he could see flowing down the green inland mountains.", "statement": "The village is not named after the settling explorer.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 20, "c": 70, "e": 10}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "97011c", "context": "Expectations that the ANC would oversee land reform--returning land seized during apartheid's forced migrations--and wealth redistribution have not been met.", "statement": "The ANC would not be in charge of land reform.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 31, "n": 31, "e": 38}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "56743e", "context": "I found her leaning against the bannisters, deadly pale.", "statement": "She couldn't stand on her own so she leaned against the bannisters until I found her.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 54, "e": 46}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "48300c", "context": "The activities included in the Unified Agenda are, in general, those expected to have a regulatory action within the next 12 months, although agencies may include activities with an even longer time frame.", "statement": "Some actions were implemented for being shorter than 12 months.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 22, "e": 41, "c": 37}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "105769c", "context": "yeah yeah i i went i went off to school wanting to either be a high school algebra teacher or high school French teacher because my two favorite people in the in high school were my algebra teacher and French teacher and uh and i was going to do that until the end of our sophomore year when we wanted uh we came time to sign up for majors and i had taken chemistry for the first time that year and surprised myself i did well in it", "statement": "You are required to sign up for a major freshman year.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 36, "c": 56, "e": 8}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "53866e", "context": "kind of kind of nothing i won't have anything to do with", "statement": "I don't want anything to do with it, no doubts about it.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 22, "c": 4, "e": 74}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "136752e", "context": "The questions may need to be tailored to", "statement": "There are some questions that may or may not need to be tailored to.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 9, "c": 8, "e": 83}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "14388e", "context": "life in prison then he's available for parole if it's if it's life and a day then he's not eligible for parole so what you know let's quit BSing with the system", "statement": "The system is corrupt because he won't be able to get parole if it's life and a day.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 39, "e": 56, "c": 5}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "103559e", "context": "A martini should be gin and vermouth and a twist.", "statement": "A martini must be composed by gin and vermouth.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 23, "e": 64, "c": 13}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "132019n", "context": "and uh really they're about it they've got a guy named Herb Williams that that i guess sort of was supposed to take the place of uh Tarpley but he uh he just doesn't have the offensive skills", "statement": "Tarpley is a better offensive player that Herb Williams.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 20, "e": 75, "n": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "118999n", "context": "that's true i didn't think about that", "statement": "You've changed my mind with a new perspective.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 71, "n": 27, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "47404e", "context": "do you really romance", "statement": "Do you really have an affair?", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 68, "c": 11, "e": 21}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "30171n", "context": "Until all members of our society are afforded that access, this promise of our government will continue to be unfulfilled.", "statement": "The government is flawed and unfulfilled.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 56, "n": 38, "c": 6}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "134514c", "context": "However, co-requesters cannot approve additional co-requesters or restrict the timing of the release of the product after it is issued.", "statement": "They will restrict timing of the release of the product.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 80, "e": 7, "n": 13}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "52542n", "context": "The long-sought, the mysterious, the elusive Jane Finn!", "statement": "Jane Finn is as beautiful as she is mysterious.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 83, "e": 16, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "47408n", "context": "the net cost of operations.", "statement": "That's how it expensive it runs.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 49, "e": 47, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "135021n", "context": "you know we keep a couple hundred dollars um if that much charged on those which isn't too bad it's just your normal", "statement": "We have money on there, which isn't great", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 50, "c": 22, "e": 28}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "88605e", "context": "The remaining parts of the north, although enticing, are difficult to explore.", "statement": "The rest of the north presents a steep challenge.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 82, "n": 17, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "98710n", "context": "well Jerry do you have a favorite team", "statement": "Jerry, do you follow any sports?", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 46, "n": 51, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "30894c", "context": "Earlier this week, the Pakistani paper Dawn ran an editorial about reports that Pakistani poppy growers are planning to recultivate opium on a bigger scale because they haven't received promised compensation for switching to other crops.", "statement": "It is illegal to grow opium in Pakistan.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 67, "c": 29, "e": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "81356e", "context": "In keeping with other early Buddhist tenets, there is no figurative representation of Buddha here, However, there is a large gilded statue from a later period inside, and behind the temple are the spreading branches and trunks of the sacred Bodhi Tree, which is said to have grown from a sapling of the first one that stood here 2,500 years ago.", "statement": "There is no statue of Buddha located there.", "label_count_round_1": {"contradiction": 2.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 66, "c": 25, "n": 9}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "132516n", "context": "right right they left a woman and a child or the cat the sheep yeah", "statement": "They were merciful in this regard, only taking the men as slaves.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 7, "n": 80, "c": 13}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "34176n", "context": "The rustic Bras-David picnic area, for example, is set alongside a burbling stream.", "statement": "The stream is always burbling.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 54, "e": 44, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "77116e", "context": "The third row of Exhibit 17 shows the Krewski, et al.", "statement": "Exhibit 17 has many rows.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 43, "n": 57}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "76947n", "context": "i think we have too thank you very much you too bye-bye", "statement": "I don't think we can thank you enough for your help.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 34, "n": 52, "c": 14}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "139635n", "context": "have that well and it doesn't seem like very many people uh are really i mean there's a lot of people that are on death row but there's not very many people that actually um do get killed", "statement": "Most people on death row end up living out their lives awaiting execution.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 77, "n": 19, "c": 4}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "101245n", "context": "we were lucky in that in one respect in that after she had her stroke she wasn't really you know really much aware of what was going on", "statement": "She had a very serious stroke.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "n": 24, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "8545c", "context": "He hadn't seen even pictures of such things since the few silent movies run in some of the little art theaters.", "statement": "He had recently seen pictures depicting those things.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 73, "n": 16, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"e": 2, "n": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "8219n", "context": "it depends a lot of uh a lot of things were thought that uh as you know the farmers thought okay we got chemicals we're putting chemicals on the field well the ground will naturally filter out the", "statement": "The farming chemicals are filtered by the ground.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 49, "n": 48, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "115247c", "context": "oh really yeah so he he's uh he's probably going to be going to jail and and the problem with him is he's on a guaranteed salary like for three years so whether he plays or not they've got to pay him ten million dollars so if they", "statement": "He is so hardworking and has helped the team achieve so much, I don't see anything wrong with paying him a million dollar salary.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 42, "c": 54, "e": 4}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "99708e", "context": "It was made up to look as much like an old-fashioned steam train as possible.", "statement": "It was built in the modern era to look like something built in the past.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 85, "n": 14, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "82415n", "context": "Then he sobered.", "statement": "He was drunk.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 40, "c": 38, "n": 22}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "82700c", "context": "During his disastrous campaign in Russia, he found time in Moscow to draw up a new statute for the Com??die-Francaise (the national theater), which had been dissolved during the Revolution.", "statement": "Russia has been successfully invaded hundreds of times.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 76, "c": 23, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3, "n": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "1073c", "context": "News ' cover says the proliferation of small computer devices and the ascendance of Web-based applications are eroding Microsoft's dominance.", "statement": "Microsoft is a more profitable company than Apple.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 83, "e": 4, "c": 13}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "120149c", "context": "There's a lot of villas all the way along, but by degrees they seemed to get more and more thinned out, and in the end we got to one that seemed the last of the bunch.", "statement": "There were only a few villas the whole way along, until we reached a small village that seemed to be the end.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 22, "c": 69, "n": 9}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "124037n", "context": "The park was established in 1935 and was given Corbett's name after India became independent.", "statement": "The park changed names due to the independence.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 4, "e": 69, "n": 27}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "92062c", "context": "Krugman's column will henceforth be known as The Dismal Science, a phrase too famous to be ownable by anyone, except possibly British essayist Thomas Carlyle (1795-1881), who coined it.", "statement": "Krugman writes novels.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 30, "n": 64, "e": 6}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "131235c", "context": "Even if the entire unified surplus were saved, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years.", "statement": "Even if the entire unified surplus were lost, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 51, "n": 17, "e": 32}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "46820e", "context": "and my and my part-time work you know it's not our the restaurant our favorite restaurant in the town of Salisbury where actually we live you know where my where i'll return to my job or whatever we can normally eat out for um under fourteen dollars", "statement": "My first part time job was in a restaurant in Salisbury where you could eat out for under $14.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 34, "n": 55, "c": 11}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "77590c", "context": "do you think most states have that or", "statement": "I think most states have that.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 70, "e": 19, "c": 11}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "128542e", "context": "There should be someone here who knew more of what was going on in this world than he did now.", "statement": "He knew things, but hoped someone else knew more.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 82, "n": 18}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "74768e", "context": "She admits to Dorcas, 'I don't know what to do; scandal between husband and wife is a dreadful thing.' At 4 o'clock she has been angry, but completely mistress of herself.", "statement": "She had remained in control despite her anger.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 57, "n": 32, "c": 11}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "122928e", "context": "A small page-boy was waiting outside her own door when she returned to it.", "statement": "When she came back to her door she found something waiting.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": ["entailment"], "has_ambiguity": true, "chaosnli_labels": {"n": 10, "c": 13, "e": 77}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "139409e", "context": "Then, all the time, it was in the spill vase in Mrs. Inglethorp's bedroom, under our very noses? I cried.", "statement": "You mean we were so near it constantly?", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 78, "n": 21, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "59208n", "context": "He's chosen Meg Ryan.", "statement": "A possible selection would be Meg Ryan or Jon Doe.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 39, "e": 13, "n": 48}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "133597n", "context": "In manual systems, attestations, verifications, and approvals are usually shown by a signature or initial of an individual on a hard copy document.", "statement": "The only things that signatures in manual systems show are attestations, verifications, or approvals.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 52, "e": 32, "c": 16}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "80517e", "context": "This doesn't look good.", "statement": "This looks really bad.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 32, "e": 65, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "93236c", "context": "The word itself, tapa, is translated as  lid  and derives from the old custom of offering a bite of food along with a drink, the food being served on a saucer sitting on top of the glass like a lid.", "statement": "Tapas are large portions and are a very filling meal.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 68, "n": 30, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "142643c", "context": "The standard technology assumptions of scenario A were used by EIA in the development of the AEO2001 reference case projections.", "statement": "EIA used the standard technology assumptions to eliminate the AEO2001 reference case projections.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 20, "n": 19, "c": 61}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "31113e", "context": "One wag, J., wrote in to ask, Is there a difference between pests and airlines?", "statement": "J. thinks there is no difference between pests and airlines.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 55, "e": 28, "c": 17}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "34776c", "context": "We did not study the reasons for these deviations specifically, but they likely result from the context in which federal CIOs operate.", "statement": "The Context in which federal CIOs operate is no different from other CIOs.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 30, "n": 60, "e": 10}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "121422c", "context": "it it like strange that it you're right in the middle of the mountains and it's so brown and dry but boy you just didn't feel", "statement": "you are in the right part of the mountains.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 72, "c": 16, "e": 12}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "104805e", "context": "California is high", "statement": "California is hyped up!", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 72, "c": 16, "e": 12}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "115821n", "context": "The Chinese calendar was used to calculate the year of Japan's foundation by counting back the 1,260 years of the Chinese cosmological cycle.", "statement": "The calculation of Japan's year of foundation was very exact.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 41, "n": 56, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "80630e", "context": "The tree-lined avenue extends less than three blocks to the sea.", "statement": "The sea isn't even three blocks away.", "label_count_round_1": {"contradiction": 1.0, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 89, "c": 3, "n": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "138862c", "context": "Also, other sorbent-based approaches in development may prove in time to be preferable to ACI, making the use of ACI only a conservative assumption.", "statement": "Hydrogen-based approaches in development may be preferable to ACl.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 66, "e": 18, "c": 16}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "105179c", "context": "I was to watch for an advertisement in the Times.", "statement": "I looked for an ad in my mailbox.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 48, "n": 45, "e": 7}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "109876n", "context": "Text Box 2.1: Gross Domestic Product and Gross National Product 48Text Box 4.1: How do the NIPA and federal unified budget concepts of", "statement": "This text displays how GDP and GNP is calculated.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 25, "n": 45, "c": 30}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "65353n", "context": "Don't take it to heart, lad, he said kindly.", "statement": "He was trying to console the lad.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 97, "n": 3}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "16494c", "context": "It cannot be outlawed.", "statement": "It has to be made illegal.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 84, "e": 8, "n": 8}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "60732n", "context": "It started with The Wild Bunch : We sexualized violence, we made it beautiful.", "statement": "Violence is now look at in the positive due to The Wild Bunch.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 70, "n": 28, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "88646c", "context": "You see, he said sadly, \"you have no instincts.\"", "statement": "He said that I had no willpower.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 80, "n": 13, "e": 7}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "53074n", "context": "ooh it's kind of tough to think of some of the others although i do watch some of some of those frivolous things uh like on Thursday nights at nine o'clock when i get home from aerobics i will watch uh Knots Landing", "statement": "I only watch frivolous things on Thursday nights.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 69, "e": 23, "c": 8}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "61818n", "context": "Kutchins and Kirk cite a particularly amusing example of such  Robert Spitzer, the man in charge of DSM-III , was sitting down with a committee that included his wife, in the process of composing a criteria-set for Masochistic Personality Disorder--a disease that was suggested for, but never made it into, the DSM-III-R (a revised edition).", "statement": "DSM-III-R is a book of personality disorders.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 32, "e": 63, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "46003n", "context": "trying to keep grass alive during a summer on a piece of ground that big was expensive", "statement": "The watering and fertilizer, can cost a lot to keep grass alive in the summer months.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 30, "e": 69, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "6386n", "context": "isn't it i can remember i've only been here eight years but i can remember coming to work from i used to live in Wylie and i could see downtown Dallas", "statement": "Downtown Dallas was a short drive from where I lived in Wylie.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 58, "n": 41, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "98739n", "context": "The living is not equal to the Ritz, he observed with a sigh.", "statement": "The living is nothing compared to the glamour of the Ritz, he said sadly.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 71, "n": 22, "c": 7}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "108624c", "context": "Exhibit 3 presents total national emissions of NOx and SO2 from all sectors, including power.", "statement": "In Exhibit 3 there are the total regional emissions od NOx and SO2 from all sectors.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 42, "e": 47, "n": 11}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "106013c", "context": "Ca'daan heard the Kal grunt and felt the horse lift.", "statement": "The Kal heard Ca'daan grunt.", "label_count_round_1": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 75, "n": 16, "e": 9}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "37407n", "context": "5 are highly correlated during summer months in some areas.", "statement": "Six are correlated to winter in certain areas.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 63, "c": 37}, "error_llm": ["entailment", "contradiction"], "not_validated_exp_llm": {"e": 2, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "70590c", "context": "China's civil war sent distressing echoes to Hong Kong.", "statement": "Japan fought a civil war.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 49, "e": 1, "c": 50}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "23901e", "context": "Then Shuman claims that Linux provides no graphical user interface.", "statement": "They made accusations about the platform.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 8, "n": 22, "e": 70}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "23280n", "context": "Sphinxes were guardian deitiesinEgyptianmythologyandthis was monumentalprotection,standing73 m (240 ft)longand20 m (66 feet) high.", "statement": "Sphinxes were put in the tombs to protect the dead.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 70, "e": 24, "c": 6}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "135247c", "context": "The original wax models of the river gods are on display in the Civic Museum.", "statement": "They have models made out of clay.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 57, "n": 38, "e": 5}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "58357c", "context": "What changed?", "statement": "Nothing changed.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 76, "c": 20, "e": 4}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "19803e", "context": "But there's SOMETHING.", "statement": "Surely there's something.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 90, "n": 8, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "144753n", "context": "When he's ready for a major strike, how many innocents do you suppose are going to suffer? To quote one of your contemporaries; 'The needs of the many outweigh the needs of the few.' '", "statement": "He won't care how many innocent people will suffer.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 49, "n": 45, "c": 6}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "15100e", "context": "but uh that has been the major change that we have noticed in gardening and that's about the extent of what we've done just a little bit on the patio and uh and waiting for the the rain to subside so we can  mow we after about a month we finally got to mow this weekend", "statement": "We have not done much gardening yet because of the rain.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 12}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "28507n", "context": "It is, as you see, highly magnified.", "statement": "It is plain for you to see that it is amplified.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 76, "c": 10, "n": 14}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "123748n", "context": "There are many such at the present time.", "statement": "There are over two currently.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 82, "n": 15, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "111338c", "context": "He threw one of them and shot the other.", "statement": "He kept his gun holstered.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 90, "n": 7, "e": 3}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "52171c", "context": "For such a governmentwide review, an entrance conference is generally held with applicable central agencies, such as the Office of Management and Budget (OMB) or the Office of Personnel Management.", "statement": "An entrance conference is held with specialized agencies.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 84, "n": 8, "c": 8}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "90548e", "context": "Splendid!", "statement": "The speaker is excited by the situation.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 72, "n": 27, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "81842c", "context": "Answer? said Julius.", "statement": "Julius already knew the answer.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 71, "c": 23, "e": 6}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "91601n", "context": "Even today, Yanomamo men raid villages, kill men, and abduct women for procreative purposes.", "statement": "Yanomamo eats food.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 8, "n": 71, "c": 21}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "118415n", "context": "John Panzar has characterized street delivery as a bottleneck function because a single firm can deliver to a recipient at a lower total cost than multiple firms delivering to the same customer.", "statement": "John Panzar believes in nationalizing all postal delivery services and couriers into a single entity for cost-saving purposes.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 42, "c": 5, "e": 53}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "113644n", "context": "so do you have do you have the long i guess not not if there's see i was raised in New York but i guess up there you all don't have too long of a growing season do you", "statement": "I am looking for a written guide to growing plants in different places in the country.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 70, "c": 22, "e": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "47260n", "context": "The good news, however, can be found in reports like this one.", "statement": "The good news is that the puppy's life was able to be saved.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 86, "c": 11, "e": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "129081e", "context": "right oh they've really done uh good job of keeping everybody informed of what's going on sometimes i've wondered if it wasn't almost more than we needed to know", "statement": "After sharing all information with everyone, I think I may have shared too much.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 63, "n": 20, "c": 17}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "18189e", "context": "The important thing is to realize that it's way past time to move it.", "statement": "It has not been moved yet in the past.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 56, "e": 38, "c": 6}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "91797c", "context": "We know they will have to come from the south but that gives them a space as wide as the town in which to launch their attack.", "statement": "The south is totally protected against an attack.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 61, "n": 37, "e": 2}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "917c", "context": "eligible individuals and the rules that apply if a state does not substantially enforce the statutory requirements.", "statement": "It does not matter whether or not a state enforces the statutory requirements.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 46, "e": 11, "c": 43}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "89995c", "context": "yeah then you don't have you don't have that mess to clean up when you use an oil oil base painting and boy i'll tell you oh", "statement": "Typically oil based paints are easy to work with and clean up.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 65, "n": 19, "c": 16}, "error_llm": ["entailment", "neutral"], "not_validated_exp_llm": {"e": 2, "n": 2}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "33822e", "context": "Why shouldn't he be?", "statement": "There is no reason he shouldn't be.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 68, "n": 27, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "42983e", "context": "The town is also known for its sparkling wine and for the caves where about 70 per?\u00adcent of France's cultivated mushrooms are grown.", "statement": "The town has a lot of sparkling wine.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 72, "n": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "6105e", "context": "Asked about abortion the other day on CNN, Republican National Committee Chairman Jim Nicholson also invoked what is apparently the party-line  inclusive party.", "statement": "The Republican National Committee Chairman gave the party's standard answer on the subject of abortion when he was asked about it on CNN.", "label_count_round_1": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 22, "e": 74, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "26143n", "context": "However, the associated cost is primarily some of the costs of assessing and collecting duties on imported merchandise, such as the salaries of import specialists (who classify merchandise) and the costs of processing paperwork.", "statement": "the associated cost is how much people spend relative to this amount", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 24, "c": 39, "n": 37}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "100768n", "context": "well in a way you can travel light", "statement": "You won't need to pack much.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 85, "n": 15}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "82510e", "context": "although the uh it's uh it we almost one day we painted the house to uh we painted we painted the whole inside and it had all this dark trim we thought uh you know we did the one wall but the other trim i'm trying to think i think i think we left most of it because it gets to be uh they don't do that in the newer houses now we don't the uh mold everything is white in a new house everything is white", "statement": "We painted the house over the duration of one day.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 39, "e": 53, "c": 8}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "102563n", "context": "The judge gave vent to a faint murmur of disapprobation, and the prisoner in the dock leant forward angrily.", "statement": "The judge ordered the court to be silent.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 45, "c": 31, "e": 24}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "48553c", "context": "Keep your eyes open for Renaissance details, grand doorways, and views into lovely courtyards.", "statement": "All of the doorways and courtyards have been completely remodeled since the Renaissance.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 53, "c": 44, "e": 3}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "54458n", "context": "This one ended up being surprisingly easy!", "statement": "This question was very easy to answer.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 81, "n": 19}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "17576n", "context": "The percent of total cost for each function included in the model and cost elasticity (with respect to volume) are shown in Table 1.", "statement": "Table 1 also shows a picture diagram for each function.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 64, "e": 27, "c": 9}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "103431n", "context": "In addition, the senior executives at these organizations demonstrated their sustained commitment to financerelated improvement initiatives by using key business/line managers to drive improvement efforts, attending key meetings, ensuring that the necessary resources are made available, and creating a system of rewards and incentives to recognize those who support improvement initiatives.", "statement": "This system of rewards and incentives will hopefully improve company performance.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 83, "n": 16, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "64123c", "context": "Per week?", "statement": "Every day.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 24, "c": 74, "e": 2}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "56163c", "context": "She would be almost certainly sent to you under an assumed one.", "statement": "The man told the other man that Bill would be sent to him.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 60, "n": 38, "e": 2}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "11297c", "context": "Transforming Control of Public Health Programs Raises Concerns (", "statement": "Everyone is content with the change of public health programs.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 24, "c": 76}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "32889n", "context": "Extremely limited exceptions to the authority are established in 31 U.S.C.", "statement": "They were trying to eliminate all exceptions.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 62, "c": 28, "e": 10}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "19c", "context": "On the northern slopes of this rocky outcropping is the site of the ancient capital of the island, also called Thira, which dates from the third century b.c. (when the Aegean was under Ptolemaic rule).", "statement": "Is the site of the ancient asteroid impact, also called Thira.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 59, "e": 8, "c": 33}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "32754e", "context": "After shuttering the DOE, Clinton could depict himself as a crusader against waste and bureaucracy who succeeded where even Reagan failed.", "statement": "Clinton shuttered the DOE to move against waste.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 18, "e": 80, "c": 2}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "12815n", "context": "yeah well that's my uh i mean every time i've tried to go you know it's always there's there's always a league bowling", "statement": "Every time I try to go bowling there are leagues only and I can't bowl.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 56, "n": 44}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "135898c", "context": "The end is near!  Then a shout went up, and Hanson jerked his eyes from the gears to focus on a group of rocs that were landing at the far end of the camp.", "statement": "It's all over, Hanson whispered as he stared at the gears.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 44, "n": 44, "e": 12}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "82830n", "context": "In the 19th century, when Kashmir was the most exotic hill-station of them all, the maharaja forbade the British to buy land there, so they then hit on the brilliant alternative of building luxuriously appointed houseboats moored on the lakes near Srinagar.", "statement": "The maharaja allowed the British to build houseboats on the lakes.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 60, "n": 20, "c": 20}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "74509n", "context": "Under the budget deal, by 2002, national defense will consume about $273 billion a year compared with $267 billion now.", "statement": "The United States national defense budget will increase by 6 billion dollars.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 87, "c": 9, "n": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "98944c", "context": "evaluation questions.", "statement": "Only statements of the evaluation are available.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 33, "n": 55, "e": 12}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "19208c", "context": "Hearty Sabbath meals.", "statement": "Hearty meals will only be offered to Buddhists", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 52, "n": 47, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "117576e", "context": "but i don't know you know  maybe you could do that for a certain period of time but i mean how long does that kind of a thing take you know to to um say to question the person or to get into their head", "statement": "It might take a long time to do that because getting inside a person's head takes time.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 69, "n": 28, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "50484c", "context": "All of our many earnest experiments produced results in line with random chance, they conclude.", "statement": "The experiments proved it was a much better predictor.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 64, "n": 28, "e": 8}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "98445c", "context": "It seeks genuine direct elections after a period that is sufficient to organize alternative parties and prepare a campaign based on freedom of speech and other civil rights, the right to have free trade unions, the release of more than 200 political prisoners, debt relief, stronger penalties for corruption and pollution, no amnesty for Suharto and his fellow thieves, and a respite for the poor from the hardest edges of economic reform.", "statement": "The only thing that can our society is more power to the presidential electors.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 17, "e": 4, "n": 79}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "7856c", "context": "but how do you know  the good from the bad", "statement": "Why care if it's good or bad?", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 64, "c": 28, "e": 8}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "8257n", "context": "'But if White has any designs at all on living, he'll be as far from Little as he can possibly get by now.'", "statement": "White should be afraid to come back to Little.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 74, "n": 24, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "141110n", "context": "yes well yeah i am um actually actually i think that i at the higher level education i don't think there's so much of a problem there it's pretty much funded well there are small colleges that i'm sure are struggling", "statement": "Small colleges usually have trouble with funding and resources.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 43, "e": 54, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "84055n", "context": "Even if auditors do not follow such other standards and methodologies, they may still serve as a useful source of guidance to auditors in planning their work under GAGAS.", "statement": "GAGAS requires strict compliance for auditors to follow.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 52, "c": 29, "e": 19}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "143789n", "context": "What a brilliantly innocuous metaphor, devised by a master manipulator to obscure his manipulations.", "statement": "The metaphor was created by the manipulator to convince people of something.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 7, "c": 5}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "13911n", "context": "Changes in technology and its application to electronic commerce and expanding Internet applications will change the specific control activities that may be employed and how they are implemented, but the basic requirements of control will not have changed.", "statement": "Technology will make it so we have less control of activities.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 35, "c": 52, "e": 13}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "122062n", "context": "The order was founded by James VII (James II of England) and continues today.", "statement": "Kings frequently founded orders that can still be found today.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 63, "e": 36, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "49396e", "context": "The road along the coastline to the south travels through busy agricultural towns and fishing villages untouched by tourism.", "statement": "There are no tourists on the road through the agricultural towns and fishing villages.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 72, "n": 24, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "73444n", "context": "well they're so close to an undefeated undefeated season they can taste it and they wanna make history so i don't think they're gonna lack for motivation", "statement": "Unless they suffer any losses, they'll remain motivated.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 41, "e": 56, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "79106e", "context": "The woman rolled and drew two spears before the horse had rolled and broken the rest.", "statement": "They were in rotation on the ground grabbing their weapons.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 51, "e": 33, "c": 16}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "82230e", "context": "However, the other young lady was most kind.", "statement": "I received a warm welcome from the other young lady who was present.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 2, "e": 64, "n": 34}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "14280n", "context": "The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.", "statement": "The hunches provided by the author weren't realistic as it pertains to agency management.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 64, "e": 6, "c": 30}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "19668c", "context": "okay and and i think we just hang up i don't think we have to do anything else", "statement": "We need to wait until they tell us what to do.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 38, "c": 51, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "12562n", "context": "David Cope, a professor of music at the University of California at Santa Cruz, claims to have created a 42 nd Mozart symphony.", "statement": "Music Professor David Cope who specializes in Mozart's music claims to have created Mozart's 42nd symphony.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 30, "e": 65, "c": 5}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "111693e", "context": "The conspiracy-minded allege that the chains also leverage their influence to persuade the big publishers to produce more blockbusters at the expense of moderate-selling books.", "statement": "Big publishers want to produce more high budget films, even if that means badly selling books.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": ["entailment"], "has_ambiguity": true, "chaosnli_labels": {"n": 33, "e": 43, "c": 24}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "124839c", "context": "(A bigger contribution may or may not mean, I really, really support Candidate X.) Freedom of association is an even bigger stretch--one that Justice Thomas would laugh out of court if some liberal proposed it.", "statement": "A bigger contribution means to support candidate Y.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 51, "n": 48, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "72870n", "context": "Because marginal costs are very low, a newspaper price for preprints might be as low as 5 or 6 cents per piece.", "statement": "Many people consider these prices to be unfair to new printers.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 91, "e": 2, "c": 7}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "129601n", "context": "Took forever.", "statement": "Lasted two years", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 47, "n": 53}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "26372n", "context": "Just like we have hairpins and powder-puffs.\" Tommy handed over a rather shabby green notebook, and Tuppence began writing busily.", "statement": "Tommy handed Tuppence an empty shabby green notebook.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 44, "n": 54, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "74534n", "context": "And far, far away- lying still on the tracks- was the back of the train.", "statement": "The train wasn't moving but then it started up.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 54, "c": 45, "e": 1}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "63469c", "context": "It lacked intelligence, introspection, and humor--it was crass, worthy of Cosmopolitan or Star . I do have a sense of humor, but can only appreciate a joke when it starts with a grain of truth.", "statement": "The article won a Pulitzer Prize.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 67, "c": 31, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "141321n", "context": "It will be held in the Maryland woods, and the telecast will consist of jittery footage of the contestants' slow descent into madness as they are systematically stalked and disappeared/disqualified by Bob Barker.", "statement": "The show will be set in the woods north of Boston.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 25, "e": 12, "c": 63}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "68946c", "context": "It has served as a fortress for the Gallo-Romans, the Visigoths, Franks, and medieval French (you can see the layers of their masonry in the ramparts).", "statement": "The fortress was built by the medieval French in 1173.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 20, "n": 79, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "41975c", "context": "Tommy realized perfectly that in his own wits lay the only chance of escape, and behind his casual manner he was racking his brains furiously.", "statement": "He'd been stuck for hours, starting to feel doubt crawl into his mind.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 70, "c": 16, "e": 14}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "100349e", "context": "He touched it and felt his skin swelling and growing hot.", "statement": "His skin was burning.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 64, "n": 35, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "107399c", "context": "Bush the elder came of age when New England Republicans led the party, and patrician manners were boons to a Republican.", "statement": "New England Republicans were weak.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 30, "n": 68, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "131623c", "context": "In the depths of the Cold War, many Americans suspected Communists had infiltrated Washington and were about to subvert our democracy.", "statement": "Communists assisted America's government during the Cold War.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 57, "n": 40, "e": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "58954c", "context": "For an authentic feel of old Portugal, slip into the cool entrance hall of theimpressive Leal Senado ( Loyal Senate building), a fine example of colonial architecture.", "statement": "All that remains of  Leal Senado is old ruins.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 43, "c": 54, "e": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "74377e", "context": "no chemicals and plus then you can use it as a fertilizer and not have to worry about spreading those chemicals like on your lawn or your bushes or whatever", "statement": "We don't want to use chemicals on our lawn", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 29, "e": 69, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "130680e", "context": "We also have found that leading organizations strive to ensure that their core processes efficiently and effectively support mission-related outcomes.", "statement": "Leading organizations want to be sure their processes are successful.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 91, "n": 9}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "49227n", "context": "well that's uh i agree with you there i mean he didn't have the surrounding cast that Montana had there's no doubt about that", "statement": "I agree that he didn't have the same support as Montana, but he did well.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 40, "n": 57, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "124853e", "context": "H-2A agricultural workers are required to maintain a foreign residence which they have no intention of abandoning.", "statement": "Permanent foreign residence is required for some types of agricultural work visas.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 84, "n": 16}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "117892n", "context": "No, Dave Hanson, you were too important to us for that.", "statement": "No, Dave Hanson, we couldn't risk your life becaus you are too important to us.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 54, "e": 46}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "111243n", "context": "The pope, suggesting that Gen.", "statement": "Gen is being suggested by the Pope.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 78, "n": 21, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "77654e", "context": "but there's no uh inscriptions or or dates or anything else", "statement": "There aren't any dates on it?", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 87, "n": 11, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "110061n", "context": "If you have the energy to climb the 387 steps to the top of the south tower, you will be rewarded with a stunning view over the city.", "statement": "The south tower has the best view in the city.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 73, "e": 26, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "29844c", "context": "I am glad she wasn't, said Jon.", "statement": "Jon was sad that she wasn't happy.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 46, "n": 54}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "27335e", "context": "is there still that type of music available", "statement": "Is that genre of music still a thing?", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 15, "e": 84, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "87332c", "context": "Strange as it may seem to the typical household, capital gains on its existing assets do not contribute to saving as measured in NIPA.", "statement": "NIPA considers cat fur when it defines savings.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 53, "n": 44, "e": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "53499c", "context": "my goodness it's hard to believe i didn't think there was anybody in the country who hadn't seen that one", "statement": "I thought I was the only one in this country who had seen it.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 23, "c": 68, "n": 9}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "65199n", "context": "and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn't need it i shouldn't have even qualified to get it i didn't need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we're paying that back i told my son if you have to live in the ghetto to go to college do it but don't take out ten thousand dollars in loans don't do it and i don't i hope don't think he'll have to do that but i just so like we might if we didn't have those loans we could have saved in the last five years the money for that and i believe we would have because God's really put it in our heart not to get in debt you know but we have friends at church that do this on a constant basis that are totally debt free and they pay cash for everything they buy", "statement": "I am envious of all my debt-free churchgoing friends.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 43, "n": 52, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "88050c", "context": "If you have any questions about this report, please contact Henry R. Wray, Senior Associate General Counsel, at (202) 512-8581.", "statement": "Henry R. Wray can be reached at (555) 512-8581.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 21, "c": 76, "n": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "34094e", "context": "No, monsieur.", "statement": "The speaker is answering no to a question.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 11, "e": 88, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "72875e", "context": "The policy succeeded, and I was fortunate to have had the opportunity to make that contribution to my people.", "statement": "Because the policy was a success, I was able to make a contribution to my people.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 11, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "19921n", "context": "3) The gap between the productivity of women and the productivity of men.", "statement": "The gap of genders.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 83, "n": 15, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "23583e", "context": "While obviously constrained by their bondage, blacks nonetheless forged a culture rich with religious observances, folk tales, family traditions, song, and so on.", "statement": "Clearly are constrained by their folk tales and traditions.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 52, "e": 14, "n": 34}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "91106n", "context": "SSA is also seeking statutory authority for additional tools to recover current overpayments.", "statement": "SSA wants the authority to recover overpayments made to insurers.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 36, "e": 61, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "9557e", "context": "Tommy Thompson of Wisconsin and Mayor Rudolph Giuliani of New York, the conservative vanguard on the issue, show no inclination to exploit research that says, in effect, Why care about day-care quality?", "statement": "Thompson and Giuliani don't want to care about day cares.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 38, "e": 46, "c": 16}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "15537n", "context": "So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.", "statement": "Our clients can barely afford our legal assistance.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 66, "n": 26, "c": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "3476n", "context": "apparently apparently the appraisers likes it because our taxes sure is high  isn't it it really is", "statement": "We wished the taxes were lower.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 31, "n": 60, "c": 9}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "56124n", "context": "Of how, when tea was done, and everyone had stood,He reached for my head, put his hands over it,And gently pulled me to his chest, which smelledOf dung smoke and cinnamon and mutton grease.I could hear his wheezy breathing now, like the prophet's Last whispered word repeated by the faithful.Then he prayed for what no one had time to translate--His son interrupted the old man to tell him a groupOf snake charmers sought his blessing, and a blind thief.The saint pushed me away, took one long look,Then straightened my collar and nodded me toward the door.", "statement": "When tea was done, he put his hands on me romantically.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": ["entailment"], "has_ambiguity": true, "chaosnli_labels": {"n": 30, "c": 39, "e": 31}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "5193n", "context": "EPA estimates that 5.6 million acres of lakes, estuaries and wetlands and 43,500 miles of streams, rivers and coasts are impaired by mercury emissions.", "statement": "The release of mercury has an impact on rivers, streams and lakes", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 99, "n": 1}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "106390n", "context": "Mykonos has had a head start as far as diving is concerned because it was never banned here (after all, there are no ancient sites to protect).", "statement": "Protection of ancient sites is the reason for diving bans in other places.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 32, "e": 60, "c": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "54327n", "context": "substitute my my yeah my kid'll do uh four or five hours this week for me no problem", "statement": "I just can't make the time because of my job.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 57, "c": 42, "e": 1}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "2870n", "context": "Most menu prices include taxes and a service charge, but it's customary to leave a tip if you were served satisfactorily.", "statement": "Most customers will tip in addition to the tax on the menus.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 74, "n": 26}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "63218n", "context": "Recently, however, I have settled down and become decidedly less experimental.", "statement": "I have lost my experimental nature due to old age.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 74, "e": 23, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "144753c", "context": "When he's ready for a major strike, how many innocents do you suppose are going to suffer? To quote one of your contemporaries; 'The needs of the many outweigh the needs of the few.' '", "statement": "He won't do a big strike because of the innocent people.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 55, "c": 35, "e": 10}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "73191n", "context": "To get a wonderful view of the whole stretch of river, and to stretch your legs in a beautiful parklike setting, climb up to the Ceteau de Marqueyssac and its jardins suspendus (hanging gardens).", "statement": "You will enjoy stretching your legs as you climb the Ceteau de Marqueyssac.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 34, "e": 61, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "62273n", "context": "The book is a parody of Bartlett's , serving up quotes from Lincoln, Jefferson, and Roger Rosenblatt with equal pomposity.", "statement": "Bill Reilly's book has quotes from various presidents ranging from Lincoln to Jefferson.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 46, "n": 46, "c": 8}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "58016c", "context": "(As the old saying goes, If you can't figure out who the fool is at the poker table, it's probably you.", "statement": "Dealers say everyone is smart that is playing.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 37, "c": 59, "e": 4}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "45605n", "context": "They have prominent red protuberances and may have been named after the British redcoats.", "statement": "They were named after the redcoats because they are the same bright red color on their bodies.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 50, "n": 48, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "82174e", "context": "NEH-supported exhibitions were distinguished by their elaborate wall panels--educational maps, photomurals, stenciled treatises--which competed with the objects themselves for space and attention.", "statement": "The exhibitions seem well-funded due to the elaborate detail of the gallery.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 46, "e": 52, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "123703e", "context": "Specifically, by defining mission improvement objectives, senior executives determine whether their organization needs a CIO who is a networking/marketing specialist, business change agent, operations specialist, policy/oversight manager, or any combination thereof.", "statement": "A CIO must be an operations specialist.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 42, "e": 30, "n": 28}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "127410n", "context": "In this case, shareholders can pay twice for the sins of others.", "statement": "shareholders can pay once for the sins of others.", "label_count_round_1": {"contradiction": 2.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 12, "c": 77, "n": 11}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "55572n", "context": "But they also don't seem to mind when the tranquillity of a Zen temple rock garden is shattered by recorded announcements blaring from loudspeakers parroting the information already contained in the leaflets provided at the ticket office; when heavy-metal pop music loudly emanates from the radio of the middle-aged owner of a corner grocery store; and when parks, gardens, and hallowed temples are ringed by garish souvenir shops whose shelves display both the tastefully understated and the hideously kitsch.", "statement": "A Zen temple rock garden is a a place for lots of people to gather and celebrate.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 26, "e": 24, "n": 50}, "error_llm": [], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "45774c", "context": "According to a 1995 Financial Executives Research Foundation report,5 transaction processing and other routine accounting activities, such as accounts payable, payroll, and external reporting, consume about 69 percent of costs within finance.", "statement": "The financial world would be ok it there wasn't any 5 percent processing.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 63, "c": 31, "e": 6}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "130869n", "context": "Castlerigg near Keswick is the best example.", "statement": "A good example would be Castlerigg near Keswick, in Scotland.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 77, "n": 21, "c": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "93955n", "context": "The large scale production of entertainment films is a phenomenon well worth seeing several times.", "statement": "The production of entertainment films is elaborate and large scaled.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 73, "n": 26, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "72870c", "context": "Because marginal costs are very low, a newspaper price for preprints might be as low as 5 or 6 cents per piece.", "statement": "Newspaper preprints can cost as much as $5.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 33, "c": 64, "e": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "77025c", "context": "You are sure that you did not in any way disclose your identity?\" Tommy shook his head.", "statement": "I wish you hadn't revealed your identity, that was a mistake.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 44, "e": 11, "c": 45}, "error_llm": [], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "69975n", "context": "I'm not interested in tactics, Al.", "statement": "Al is very interested in tactics.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 59, "c": 33, "e": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "125238c", "context": "If the collecting entity transfers the nonexchange revenue to the General Fund or another entity, the amount is accounted for as a custodial activity by the collecting entity.", "statement": "Nonexchange revenue to the General Mills.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 12, "c": 39, "n": 49}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "27022c", "context": "For fiscal year 1996, Congress determined that the Commission should recover $126,400,000 in costs, an amount 8.6 percent higher than required in fiscal year 1995.", "statement": "Congress determined that Commission should recover over $126 in costs.", "label_count_round_1": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 43, "e": 56, "n": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "66689c", "context": "OMB issued the guidance in Memorandum M0010, dated April 25, 2000.", "statement": "Memorandum M0010 was issued by INS.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 87, "n": 11, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "112547n", "context": "Credibility is a vital factor, and Jim Lehrer does, indeed, have it.", "statement": "Everyone would believe whatever Jim Lehrer said.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 49, "n": 47, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "129081n", "context": "right oh they've really done uh good job of keeping everybody informed of what's going on sometimes i've wondered if it wasn't almost more than we needed to know", "statement": "I think I have shared too much information with everyone, so next year I will share less.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 67, "c": 19, "e": 14}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "45306n", "context": "Each caters to a specific crowd, so hunt around until you find the one right for you.", "statement": "There are marketers who have argued that there needs to be more effort to broaden appeal.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 30, "n": 68, "e": 2}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "76957c", "context": "Both initial and supplemental proposed rule publications invited comments on the information collection requirements imposed by the rule.", "statement": "There's no point in following politics or voting because your vote won't actually make a difference.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 65, "c": 33, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "2262n", "context": "She buried his remains to spare her mother the gruesome sight.", "statement": "The remains would have caused grief to her mother.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 90, "n": 9, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "40710n", "context": "Write, write, and write.", "statement": "You should keep practicing writing.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 76, "n": 24}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "21957n", "context": "But those that are manufactured for sale in in Europe and so forth are quite the other way around", "statement": "Products are made with differently designed machines in Europe.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 24, "n": 70, "c": 6}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "14126e", "context": "and so i have really enjoyed that but but there are i do have friends that watch programs like they want to see a particular program and they are either home watching it or definitely recording it they have some programs that they won't miss", "statement": "What programs do your friends like to watch?", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 68, "c": 20, "e": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "79013n", "context": "But it just might be because he's afraid he'll lose his No.", "statement": "He's definitely afraid of losing he's No.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment", "neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 59, "c": 17, "e": 24}, "error_llm": [], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "38156n", "context": "BUDGETARY RESOURCES - The forms of authority given to an agency allowing it to incur obligations.", "statement": "Administrations generally feel that some agencies should have more budgetary resources than others.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 79, "e": 14, "c": 7}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "25304n", "context": "well we bought this with credit too  well we found it with a clearance uh down in Memphis i guess and uh", "statement": "We bought non-sale items in Memphis on credit.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 57, "n": 20, "e": 23}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "111338e", "context": "He threw one of them and shot the other.", "statement": "He shot his gun.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 37, "e": 63}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "75259c", "context": "Buffet and a\u00a0 la carte available.", "statement": "It has table service.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 47, "e": 36, "c": 17}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "53211n", "context": "No, I exclaimed, astonished.", "statement": "I said no to him several time, utterly surprised by the change of events.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 59, "c": 13, "e": 28}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "123267e", "context": "He's a bad lot.", "statement": "He's a dishonest person", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 57, "e": 42, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "80808n", "context": "A button on the Chatterbox page will make this easy, so please do join in.", "statement": "They wanted to make the site very user friendly.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 68, "n": 32}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "72740c", "context": "So it wasn't Missenhardt's singing--marvelous though that was--that made Osmin's rantings so thrilling.", "statement": "Osmin was always calm and collected.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 44, "c": 56}, "error_llm": ["entailment", "neutral"], "not_validated_exp_llm": {"e": 1, "n": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "71251n", "context": "Deborah Pryce said Ohio Legal Services in Columbus will receive a $200,000 federal grant toward an online legal self-help center.", "statement": "A $200,000 federal grant will be received by Ohio Legal Services, said Deborah Pryce, who could finally say it to the public.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 63, "n": 37}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "117680c", "context": "Since the rules were issued as interim rules and not as general notices of proposed rulemaking, they are not subject to the Unfunded Mandates Reform Act of 1995.", "statement": "The rules were  not issued as interim rules but rather general notices of proposed rulemaking.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 76, "e": 18, "n": 6}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "80930n", "context": "so you um-hum so you think it comes down to education or or something like that", "statement": "IT all boils down to how much education you have.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 51, "e": 45, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "98844n", "context": "The m??tro (subway) is the fastest way to move around the city, but the buses, both in the capital and the other big towns, are best for taking in the sights.", "statement": "Taking the subway is a good way to experience big city life.", "label_count_round_1": {"contradiction": 2.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 42, "n": 38, "e": 20}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "127073c", "context": "maybe adult literacy maybe you know composition writing maybe you know uh volunteering you know on a tutor line or though the even through the elementary schools for help with homework or the other part of me says is God i've had enough kids  do i really", "statement": "maybe I could volunteer to help coach sports since I've helped all my children be successful in sports", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 50, "n": 47, "e": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "61216c", "context": "By seeding packs with a few high-value cards, the manufacturer is encouraging kids to buy Pokemon cards like lottery tickets.", "statement": "Each Pokemon card pack is filled with every rare card a kid could want.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 20, "c": 63, "e": 17}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "102857n", "context": "Expenses included in calculating net cost for education and training programs that are intended to increase or maintain national economic productive capacity shall be reported as investments in human capital as required supplementary stewardship information accompanying the financial statements of the Federal Government and its component units.", "statement": "Net cost for college programs can be calculated as a way to increase productivity.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 60, "n": 35, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "133243e", "context": "He watched the river flow.", "statement": "The river roared by.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 12, "n": 66, "e": 22}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "144408n", "context": "Today it is possible to buy cheap papyrus printed with gaudy Egyptian scenes in almost every souvenir shop in the country, but some of the most authentic are sold at The Pharaonic Village in Cairo where the papyrus is grown, processed, and hand-painted on site.", "statement": "The Pharaonic Village in Cairo is the only place where one can buy authentic papyrus.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 20, "c": 41, "n": 39}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "104412e", "context": "After being diagnosed with cancer, Carrey's Kaufman decides to do a show at Carnegie Hall.", "statement": "Carrey's Kaufman was diagnosed with cancer before deciding to do a show at Carnegie Hall.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 88, "c": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "129601e", "context": "Took forever.", "statement": "Lasted too long", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 90, "n": 10}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "62238e", "context": "Clearly, GAO needs assistance to meet its looming human capital challenges.", "statement": "GAO will soon be suffering from a shortage of qualified personnel.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 61, "n": 38, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "51353e", "context": "It is not a surprise, either, that Al Pacino chews the scenery in Devil's Advocate . And the idea that if the devil showed up on Earth he'd be running a New York corporate-law firm is also, to say the least, pre-chewed.", "statement": "The fact that the devil would work in law is extremely cliche.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 16, "e": 79, "c": 5}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "108027n", "context": "The door opened and Severn stepped out.", "statement": "They were waiting for someone to open the door for them.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 73, "c": 23, "e": 4}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "95883e", "context": "Charles Geveden has introduced legislation that will increase the Access to Justice supplement on court filing fees.", "statement": "Charles Geveden initiated a law that will essentially lower court filing fees.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 47, "c": 31, "n": 22}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "26495e", "context": "Standard screens may not perform as well in these patient subgroups that may represent a considerable part of the ED population.", "statement": "The subgroups may not perform well in standard screens.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 71, "n": 15, "c": 14}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "67836n", "context": "Who are these sons of eggs?", "statement": "I wish they were daughters of eggs.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 34, "n": 65, "e": 1}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "21340n", "context": "uh somewhat they're not my favorite team i am uh somewhat familiar with them", "statement": "They are the best team in the league, by they are not my favorite.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 84, "c": 11, "e": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "114971e", "context": "They won't be killing off George Clooney's character at ER like they did to Jimmy Smits at NYPD . Instead, Dr. Doug Ross is being forced out over the next two episodes because the maverick heartthrob gives an unauthorized painkiller to a terminally ill boy (Thursday, 10 p.m.).", "statement": "George Clooney will not be getting fired from his TV show.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 36, "e": 36, "n": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "103354n", "context": "The Varanasi Hindu University has an Art Museum with a superb collection of 16th-century Mughal miniatures, considered superior to the national collection in Delhi.", "statement": "The Varanasi Hindu University has an art museum on its campus which may be superior objectively to the national collection in Delhi.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 22, "e": 78}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "11971n", "context": "In a six-year study, scientists fed dogs and other animals irradiated chicken and found no evidence of increased cancer or other toxic effects.", "statement": "Scientists gave animals irradiated chicken and they all lived as long as the rest of them.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 52, "n": 44, "c": 4}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "83722e", "context": "Whether a government postal service can engage in these kinds of negotiations deserves serious study.", "statement": "There is serious study needed to check.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 19, "e": 76, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "38527n", "context": "will never be doused (Brit Hume, Fox News Sunday ; Tony Blankley, Late Edition ; Robert Novak, Capital Gang ; Tucker Carlson, The McLaughlin Group ). The middle way is best expressed by Howard Kurtz (NBC's Meet the Press )--he scolds Brill for undisclosed campaign contributions and for overstretching his legal case against Kenneth Starr but applauds him for casting light on the media.", "statement": "They wanted the public to know where the funds came from.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 39, "n": 47, "c": 14}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "59934n", "context": "Likewise, at their production decision reviews, these programs did not capture manufacturing and product reliability knowledge consistent with best practices.", "statement": "Their production decision reviews located an anomaly in the data.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 54, "e": 30, "c": 16}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "10916n", "context": "He'd gone a long way on what he'd found in one elementary book.", "statement": "He learned a lot from that elementary book.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 88, "n": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "34043c", "context": "The Gaiety Theatre in South King Street is worth visiting for its ornate d??cor.", "statement": "The Trump Tower is a terrible place to visit for ornate decor.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 62, "c": 38}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 2, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "117089n", "context": "appropriate agency representatives, help resolve", "statement": "the right agency workers, help fix my security system", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 67, "e": 32, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "121910c", "context": "If ancient writings give only a romanticized view, they do offer a more precise picture of Indo-Aryan society.", "statement": "Ancient writings  show an accurate picture of Indo-Anryan society.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 17, "e": 60, "n": 23}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "113668n", "context": "If necessary to meeting the restrictions imposed in the preceding sentence, the Administrator shall reduce, pro rata, the basic Phase II allowance allocations for each unit subject to the requirements of section 414.", "statement": "Section 414 helps balance allowance allocations for units.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 66, "n": 34}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "128176e", "context": "The chart to which Reich refers was actually presented during Saxton's opening statement, hours before Reich testified, and did not look as Reich claims it did.", "statement": "Reich refers to a chart that he misunderstood.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 39, "e": 55, "c": 6}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "122322e", "context": "well uh normally i like to to go out fishing in a boat and uh rather than like bank fishing and just like you try and catch anything that's swimming because i've had such problems of trying to catch any type of fish that uh i just really enjoy doing the boat type fishing", "statement": "I fish in the boat and try catching any fish because I have trouble catching certain types.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 79, "n": 15, "c": 6}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "53619c", "context": "True devotees talk shop at even more specialized groups, such as one on Northeastern weather (ne.weather), whose recent conversation topics included the great blizzard of 1978 and the freak snowstorm of May 1977.", "statement": "Ne.weather is a general discussion group, not only about weather.", "label_count_round_1": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 29, "c": 64, "e": 7}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "27287c", "context": "we were talking . Try to behave", "statement": "We are having an argument, come at me if you dare!", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 58, "n": 41, "e": 1}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "125700e", "context": "Don't forget to take a change of clothing and a towel.", "statement": "Remember to replace your towel and clothing.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 57, "n": 36, "c": 7}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "96946n", "context": "Once or twice, but they seem more show than battle, said Adrin.", "statement": "Adrin said they liked to perform more than they did fight.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 21, "e": 74, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "17660e", "context": "and not only that it it opens you to phone solicitations", "statement": "It also opens the door to move marketing calls.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 62, "n": 36, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "107252c", "context": "On the northwestern Alpine frontier, a new state had appeared on the scene, destined to lead the movement to a united Italy.", "statement": "The alpine frontier was separated from Italy by glaciers.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 80, "c": 17, "e": 3}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "113967e", "context": "'I don't know what happened, exactly.' I said.", "statement": "You aren't making sense.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 73, "c": 19, "e": 8}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "129464c", "context": "It can entail prospective and retrospective designs and it permits synthesis of many individual case studies undertaken at different times and in different sites.", "statement": "It can entail prospective and retrospective designs for system redesigns.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 48, "n": 48, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "82528c", "context": "you know maybe it just wasn't possible at all in the first place you know like the no new taxes thing you know that's uh with the economy going the way it is and everything that was nearly ridiculous thing to", "statement": "it's possible to have no new taxes with the way the economy is right now.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 42, "n": 23, "e": 35}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "134356n", "context": "You will remember my saying that it was wise to beware of people who were not telling you the truth.\"", "statement": "There might be dishonest people around here.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 74, "n": 24, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "102075c", "context": "um-hum with the ice yeah", "statement": "With the sunshine and heat wave yes.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 70, "n": 30}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "n": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "96956c", "context": "You wonder whether he could win a general election coming out of the right lane of the Democratic Party.", "statement": "He will not run in a general election while he is a conservative Democrat.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 23, "n": 70, "e": 7}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "24163e", "context": "We have done that spectacularly.", "statement": "Spectacular results was the only way to describe the impact of our past work.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 31, "e": 67, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "31775c", "context": "well what station plays uh that type of music", "statement": "What TV station has documentaries about space travel?", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 74, "n": 26}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "8487n", "context": "We always knew it was an outside chance.", "statement": "We were never assured of it happening in time and we knew this full well.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 64, "c": 8, "n": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "46576e", "context": "Perhaps a further password would be required, or, at any rate, some proof of identity.", "statement": "Identity should be a minimum requirement.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 68, "n": 25, "c": 7}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "83247e", "context": "It's come back? cried Julius excitedly.", "statement": "They were excited to hear it will come back.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 37, "e": 59, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "80109n", "context": "if it had rained any more in the last two weeks instead of planting Saint Augustine grass in the front yard i think i would have plowed everything under and had a rice field", "statement": "It has rained enough to flood everything here and make rice pattys.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 24, "e": 51, "n": 25}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "52854n", "context": "This was used for ceremonial purposes, allowing statues of the gods to be carried to the river for journeys to the west bank, or to the Luxor sanctuary.", "statement": "Statues were moved to Luxor for funerals and other ceremonies.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 53, "n": 47}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "133274n", "context": "(Imagine the difference between smoking a cigarette and injecting pure nicotine directly into a vein.)", "statement": "Smoking a cigarette is a lot like injecting pure nicotine.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 17, "c": 47, "n": 36}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "120070n", "context": "well do you know you have a ten limit a ten minute time limit well that's okay and then they come on and tell you and they tell you got five seconds to say good-bye", "statement": "You get a ten minute time limit, but sometimes you'll be told to end early.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 62, "n": 33, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "129185n", "context": "Lincoln glared.", "statement": "The man was angry.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 56, "n": 42, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "49172n", "context": "These alone could have valuable uses.", "statement": "They may be valuable.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 86, "n": 13, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "140782c", "context": "Generally, FGD systems tend to be constructed closer to the ground compared to SCR technology retrofits.", "statement": "FGD systems tend to replicate SCR systems.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 33, "e": 9, "c": 58}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "45443c", "context": "This confluence of a bad tax, a $1 billion reserve, a botched opposition campaign, and voters willing to call a bluff resulted in the I-695 victory.", "statement": "The I-695 failed in its campaign to help the people.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 57, "e": 12, "n": 31}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "22938e", "context": "Despite a recent renovation, the Meadows Mall is the least appealing of the three suburban malls.", "statement": "The Meadows Mall is not appealing.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 4.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 66, "n": 31, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "67571c", "context": "Everybody has this quote from NBA commissioner David  You cannot strike your boss and still hold your job--unless you play in the NBA.", "statement": "NBA commissioner said he hates NBA players.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 55, "n": 44, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "5087e", "context": "approaches to achieving missions vary considerably between agencies.", "statement": "Approaches to achieving missions might change a lot.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 75, "n": 23, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "23769n", "context": "Kom Ombo is an unusual temple in that it is dedicated to two gods.", "statement": "Rarely visited, Kom Ombo is a strange temple devoted to two gods.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 64, "e": 36}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "13760n", "context": "If they have overestimated how far the CPI is off, Boskin and his commission may institutionalize an underestimated CPI--guaranteeing a yearly, stealth tax increase.", "statement": "If they've overestimated how far the CPI is off, it will have horrific consequences.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 62, "e": 35, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "2133n", "context": "The tomb guardian will unlock the gate to the tunnel and give you a candle to explore the small circular catacomb, but for what little you can see, it is hardly worth the effort.", "statement": "The tomb garden can give you a thorough tour of the catacombs.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 76, "e": 10, "n": 14}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "117093n", "context": "Hong Kong has long been China's handiest window on the West, and the city is unrivaled in its commercial know-how and managerial expertise.", "statement": "Hong Kong is a great place to find commercial know-how if you are hiring someone new.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 54, "n": 44, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "112402c", "context": "Although the accounting and reporting model needs to be updated, in my view, the current attest and assurance model is also out of date.", "statement": "The accounting model needs to be updated in addition to the acquisition model.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 43, "n": 44, "c": 13}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "32754n", "context": "After shuttering the DOE, Clinton could depict himself as a crusader against waste and bureaucracy who succeeded where even Reagan failed.", "statement": "Reagan had tried to shutter the DOE but was unable to.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"c": 11, "n": 25, "e": 64}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "120896n", "context": "Tell me, how did those scribbled words on the envelope help you to discover that a will was made yesterday afternoon?\" Poirot smiled.", "statement": "How did you work out from that text that there was a new will?", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 88, "n": 12}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "25437n", "context": "Well, we will come in and interview the brave Dorcas.\" Dorcas was standing in the boudoir, her hands folded in front of her, and her grey hair rose in stiff waves under her white cap.", "statement": "Dorcas is well known for her bravery.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 60, "n": 36, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "56895c", "context": "The entire economy received a massive jump-start with the outbreak of the Korean War, with Japan ironically becoming the chief local supplier for an army it had battled so furiously just a few years earlier.", "statement": "Korea and Japan were not at war.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 28, "n": 20, "e": 52}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "43094c", "context": "Time 's cover package considers what makes a good school.", "statement": "Time's cover package is about how most college students have to deal with insane student loans.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 50, "c": 49, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "21834e", "context": "But the world is not run for the edification of tourists.", "statement": "The world does not try and morally subject to tourists.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 58, "n": 38, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "139836n", "context": "The centralization dear to Richelieu and Louis XIV was becoming a reality.", "statement": "Louis XIV cared a lot about centralization of his country and people.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 66, "n": 34}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "57454e", "context": "what does um is Robby Robin Williams does he have a funny part in the movie or is", "statement": "Is Robin Williams in the movie?", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 45, "e": 40, "c": 15}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "34573n", "context": "But the door was locked?\" These exclamations burst from us disjointedly.", "statement": "We chaotically exclaimed as we all jumped up in a frenzy, \"But the door wasn't unlocked?\"", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 43, "n": 20, "c": 37}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "52278e", "context": "Tuppence rose.", "statement": "Tuppence stood up.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 11, "e": 88, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "11618c", "context": "Enlarging the village was not desirable and most knew that Severn only desired wealth and a seat on the council of elders.", "statement": "Severn was happy being poor.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 89, "n": 11}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "118460n", "context": "and the other thing is the cost it's almost prohibitive to bring it to a dealer", "statement": "The cost of fixing it makes it hard to bring it to a dealer.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 76, "n": 22, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "54383c", "context": "He knew how the Simulacra was supposed to develop.", "statement": "He didn't know about Sims.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 50, "n": 47, "e": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "123751n", "context": "i think that the people that are um have um a lower income which you automatically equate with lower education", "statement": "I think because you have lower income you are less educated.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 7, "e": 78, "n": 15}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "43891n", "context": "GAO recommends that the Secretary of Defense revise policy and guidance", "statement": "GAO recommends that you eat 5 fruit/veg per day", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 50, "n": 50}, "error_llm": ["entailment", "contradiction"], "not_validated_exp_llm": {"e": 3, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "113039e", "context": "In this respect, bringing Steve Jobs back to save Apple is like bringing Gen.", "statement": "Steve Jobs came back to Apple.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 40, "e": 42, "c": 18}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "83900e", "context": "Ca'daan closed the door behind them and retied the not.", "statement": "Ca'daan closed the door as they entered, and bound it shut with rope.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 59, "n": 33, "c": 8}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "10119e", "context": "Then he is very sure.", "statement": "He is very sure of himself.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 60, "n": 37, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "21810n", "context": "you can get a hard copy of it and that's about it", "statement": "An email won't cut it.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 46, "n": 43, "c": 11}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "35700c", "context": "The Honorable Bill Archer, Chairman The Honorable Charles B. Rangel Ranking Minority Member Committee on Ways and Means House of Representatives", "statement": "Bill Archer has never held government office in his entire life.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 85, "n": 14, "e": 1}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "109278n", "context": "Lawyers in their first three years of practice or who are inactive pay $90, and retired lawyers pay nothing.", "statement": "Lawyers pay $90 to be included in the directory.", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 74, "c": 13, "e": 13}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "138530e", "context": "It vibrated under his hand.", "statement": "It hummed quietly in his hand.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 36, "e": 43, "c": 21}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "13387e", "context": "yeah i can believe that", "statement": "I agree with what you said.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 70, "n": 28, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "58557e", "context": "In the first instance, IRS would have no record of time before the person could get through to an agent and of discouraged callers.", "statement": "There is no recording of the time for callers.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 77, "n": 16, "c": 7}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "36811c", "context": "This having come to his stepmother's ears, she taxed him with it on the afternoon before her death, and a quarrel ensued, part of which was overheard.", "statement": "A love affair sparked just moments before her death.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 7, "n": 40, "c": 53}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "73840n", "context": "Hersheimmer \"WELL,\" said Tuppence, recovering herself, \"it really seems as though it were meant to be.\" Carter nodded.", "statement": "See, luck is real!", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 70, "c": 10, "e": 20}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "31249e", "context": "(And yes, he has said a few things that can, with some effort, be construed as support for supply-side economics.)", "statement": "It would take some work to construe the things as support for supply-side economics.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 86, "c": 6, "n": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "91913n", "context": "This is one of the reasons we're growing too weak to fight the Satheri.  \"What's wrong with a ceremony of worship, if you must worship your eggshell?\" Dave asked.", "statement": "Eggshell worship is the reason we're growing too weak to fight the Satheri, yet Dave asked about it.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 60, "n": 36, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "100895c", "context": "Is there adequate information for judging generalizability?", "statement": "Every output has some kind of resource.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 78, "e": 2, "c": 20}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 4}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "837n", "context": "The central features of the Results Act-strategic planning, performance measurement, and public reporting and accountability-can serve as powerful tools to help change the basic culture of government.", "statement": "The Results Act has strategic planning as a central feature for public organizations.", "label_count_round_1": {"contradiction": 2.0, "entailment": 2.0, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 37, "e": 61, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "140005c", "context": "3 It should be noted that the toxicity (LC50) of a sample observed in a range-finding test may be significantly different from the toxicity observed in the follow-up chronic definitive test  (1) the definitive test is longer; and (2) the test may be performed with a sample collected at a different time, and possibly differing significantly in the level of toxicity.", "statement": "The toxicity of a sample in the range-finding test will be exactly the same as the toxicity in the follow-up test.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 85, "e": 5, "n": 10}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "38475c", "context": "In 1984, Clinton picked up rock groupie Connie Hamzy when she was sunbathing in a bikini by a hotel pool.", "statement": "Clinton kept her friends and relationships private in the 80s.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 65, "c": 33, "e": 2}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "141293e", "context": "oh wow no i just started about well five years ago i think", "statement": "It had started five years ago.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 31, "e": 64, "c": 5}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "30139n", "context": "oh that's not really important the the other stuff is just you know window dressing because we we've never ordered anything fact the the van that we've got we bought uh from an estate it was an estate trade uh it was almost brand new the the gentlemen who owned it had died", "statement": "We were very lucky to get the van given how new it was.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 62, "n": 35, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "30380c", "context": "The NYT , in its front-page coverage, says the plane was flying far lower than the rules for training missions allow.", "statement": "The NYT reported that training missions did allow for planes to fly that low.", "label_count_round_1": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 66, "e": 28, "n": 6}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "22436n", "context": "1 Now that each unit is fully staffed, the LSC Office of Program Performance and its state planning team contain over 260 years of experience in LSC-funded programs.", "statement": "The LSC has over 260 years of experience with their lawyers.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 45, "e": 43, "c": 12}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "76037n", "context": "You did, didn't you?\"", "statement": "You didn't mean to do that, did you?", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 49, "c": 40, "e": 11}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "28456n", "context": "A clean, wholesome-looking woman opened it.", "statement": "The woman was trying to be desecrate.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 68, "c": 31, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "46198c", "context": "How effectively DOD manages these funds will determine whether it receives a good return on its investment.", "statement": "The DOD is certain to have a bad return on these funds.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 52, "e": 3, "n": 45}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "100136e", "context": "Challenges to Restore Public Confidence in", "statement": "Public confidence can be difficult to reestablish.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 31, "e": 68, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "24103n", "context": "if the United States had used full conventional power.", "statement": "The United States is unable to maximize their potential.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 64, "c": 18, "e": 18}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "142729c", "context": "What Ellison is doing here, as Hemingway did, is equating the process of becoming an artist with that of becoming a man.", "statement": "Ellison and Hemingway took different ways to compare becoming a man.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 79, "n": 12, "e": 9}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "96516e", "context": "As Ben Yagoda writes in the New York Times Book Review , somewhere along the way, Kidder must have decided not to write a book about Tommy O'Connor.", "statement": "A book was not written about Tommy O'Connor.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 73, "n": 22, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "52761e", "context": "My unborn children will never appear on the Today show.", "statement": "No direct descendent of mine will ever be a guest of the Today show.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 34, "e": 65, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "21297n", "context": "He was crying like his mother had just walloped him.", "statement": "He was crying like his mother hit him with a spoon.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 74, "c": 2, "e": 24}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "98487c", "context": "Julius nodded gravely.", "statement": "Julius loves to ask questions.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 80, "c": 20}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "17179n", "context": "Lie back, and DON'T THINK.", "statement": "Lie back, and do not use your crazy mind.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 63, "n": 37}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "132539e", "context": "Boca da Corrida Encumeada (moderate; 5 hours): views of Curral das Freiras and the valley of Ribeiro do Poco.", "statement": "Boca da Corrida Encumeada is a moderate text that takes 5 hours to complete.", "label_count_round_1": {"contradiction": 2.0, "entailment": 2.0, "neutral": null}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 14, "n": 36, "e": 50}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "23642n", "context": "The second half of the book dealt with the use of the true name.", "statement": "The first part dealt with the use of false names.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 91, "c": 5, "e": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "84781e", "context": "By coordinating policy development and awareness activities in this manner, she helps ensure that new risks and policies are communicated promptly and that employees are periodically reminded of existing policies through means such as monthly bulletins, an intranet web site, and presentations to new employees.", "statement": "She can find new risks with the awareness campaign.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 31, "e": 57, "c": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "15771c", "context": "or just get out and walk uh or even jog a little although i don't do that regularly but Washington's a great place to do that", "statement": "\"I regularly go for a walk or a jog at Washington's.\"", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 13, "c": 76, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "134655c", "context": "Catch up on the Indian avant-garde and the bohemian people of Caletta at the Academy of Fine Arts on the southeast corner of the Maidan.", "statement": "The Academy of Fine Arts is located in Northern Maidan.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 15, "c": 77, "e": 8}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "33822n", "context": "Why shouldn't he be?", "statement": "He doesn't actually want to be that way.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 80, "c": 13, "e": 7}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "125021c", "context": "Other functional components of the Postal Service are presumed here not to exhibit significant scale economies, although this has not been demonstrated.", "statement": "The Postal Service only operates very large scale economies.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 48, "c": 48, "e": 4}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "133005n", "context": "In May 1967, Gallup found that the number of people who said they intensely disliked RFK--who was also probably more intensely liked than any other practicing politician--was twice as high as the number who intensely disliked Johnson, the architect of the increasingly unpopular war in Vietnam.", "statement": "Due to his attitudes on cheesecake, RFK was more disliked than Johnson.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 43, "n": 49, "e": 8}, "error_llm": ["entailment", "contradiction"], "not_validated_exp_llm": {"e": 2, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "65650e", "context": "She didn't listen.", "statement": "She did not listen to the noise.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 45, "e": 54, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "3545n", "context": "Several of the organizations had professional and administrative staffs that provided analytical capabilities and facilitated their members' participation in the organization's activities.", "statement": "Organizations had mandatory bonding exercises for their members.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 15, "n": 77, "c": 8}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "105196n", "context": "Indeed, said San'doro.", "statement": "They were certain.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 49, "n": 50, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "144753e", "context": "When he's ready for a major strike, how many innocents do you suppose are going to suffer? To quote one of your contemporaries; 'The needs of the many outweigh the needs of the few.' '", "statement": "If he does a big strike, many people will suffer.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 62, "n": 36, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "114458e", "context": "Mortifyingly enough, it is all  the difficulty, the laziness, the pathetic formlessness in youth, the round peg in the square hole, the whatever do you want?", "statement": "Many youth are lazy.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 68, "n": 29, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "49237c", "context": "'You burned down my house.'", "statement": "'Even though you tried to burn it down, my house is in perfect state.'", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 91, "n": 9}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "70047c", "context": "What about the hole?\" They scanned the cliff-side narrowly.", "statement": "They looked from the top of the cliff for the hole.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 33, "n": 57, "c": 10}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "137319n", "context": "And she came to you?", "statement": "The person asked if the woman came to him.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 81, "n": 18, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "63013n", "context": "Although claims data provide the most accurate information about health care use, ensuring adequate follow-up for purposes of obtaining information from patient self-report is important because many people do not report alcohol-related events to insurance compa-nies.", "statement": "The insurance companies want to reduce medical payments by following-up to ensure patient was sober at the time of incident and intoxication may lead to a claim denial on reimbursement for medical expenses.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 45, "e": 47, "c": 8}, "error_llm": ["neutral"], "not_validated_exp_llm": {"n": 3}, "label_set_llm": [], "label": [0.0, 0.0, 0.0]}
{"id": "79507e", "context": "An organization's activities, core processes, and resources must be aligned to support its mission and help it achieve its goals.", "statement": "An organization is successful if its activities, resources, and goals align.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 75, "n": 25}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "121910e", "context": "If ancient writings give only a romanticized view, they do offer a more precise picture of Indo-Aryan society.", "statement": "Ancient writings don't show an accurate picture of Indo-Anryan society.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"c": 63, "n": 18, "e": 19}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "120323n", "context": "In the original, Reich is set up by his host and then ambushed by a hostile questioner named John, and when he tries to answer with an eloquent Mr. Smith speech (My fist is clenched.", "statement": "Reich's host is out to get him.", "label_count_round_1": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "n": 22, "c": 3}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "50480n", "context": "But you will find it all right.\"", "statement": "You, I'm sure, will find it more than adequate.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 67, "n": 29, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "123027n", "context": "uh high humidity", "statement": "Warm, sweaty temperatures.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 72, "n": 26, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "85279n", "context": "The almost midtown Massabielle quarter (faubourg de Massabielle), is sometimes described as the most picturesque in the city.", "statement": "The Massabielle quarter is a very touristy place.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 78, "e": 20, "c": 2}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "121360c", "context": "The tip was hooked towards the edge, the same way the tips are hammered for knives used for slaughter.", "statement": "They were fragile and could not leave a scratch.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 52, "c": 47, "e": 1}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "56582n", "context": "So far, however, the number of mail pieces lost to alternative bill-paying methods is too small to have any material impact on First-Class volume.", "statement": "Occasionally mail is lost but not often", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 36, "e": 60, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "46650n", "context": "The draft treaty was Tommy's bait.", "statement": "Tommy took the bait of the treaty.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 56, "c": 10, "e": 34}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "81579e", "context": "All were prominent nationally known organizations.", "statement": "The only identified organizations were well-known.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 78, "n": 18, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "23414c", "context": "Why bother to sacrifice your lives for dirt farmers and slavers?", "statement": "No one cares about the dirt farmers and slaves.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 48, "e": 40, "c": 12}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "36715e", "context": "Jon twisted the man's wrist.", "statement": "Jon grabbed the man.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 76, "n": 22, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "114492n", "context": "and the same is true of the drug hangover you know if you", "statement": "It's just like a drug hangover but worse.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 23, "n": 66, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "136097n", "context": "and going to school is also always very prohibitive now unless your parents are wealthy", "statement": "Wealthy parents are necessary for school.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 28, "e": 61, "c": 11}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "101253c", "context": "In his effort to build nationalism across Turkey in the 1920s, Ataterk instituted a campaign to suppress Kurdish identity that continues today.", "statement": "In 1942, Ataterk tried to build nationalism in Turkey.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 62, "e": 22, "n": 16}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "n": 1, "c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "97520e", "context": "AC Green's pretty good", "statement": "AC Green is also a solid player.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 69, "n": 29, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "97569e", "context": "Candle grease?", "statement": "Was it candle grease?", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 72, "n": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "23751c", "context": "Part of the reason for the difference in pieces per possible delivery may be due to the fact that five percent of possible residential deliveries are businesses, and it is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses.", "statement": "We all know that the reason for a lesser percentage of possible deliveries on rural routes being businesses, is because of the fact that people prefer living in cities rather than rural areas.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 16, "n": 65, "c": 19}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "103169n", "context": "'Dave Hanson, to whom nothing was impossible.' Well, we have a nearly impossible task: a task of engineering and building.", "statement": "This building job is almost impossible, even for an experienced engineer.", "label_count_round_1": {"contradiction": 1.0, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["contradiction", "neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 72, "n": 26, "c": 2}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "134217e", "context": "uh-huh and is it true i mean is it um", "statement": "It's true.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": [], "error_labels": ["entailment", "neutral"], "has_ambiguity": false, "chaosnli_labels": {"n": 42, "e": 50, "c": 8}, "error_llm": [], "not_validated_exp_llm": {"n": 2, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "11601n", "context": "36 AC usage nationally for mercury control from power plants should be roughly proportional to the total MWe of coal-fired facilities that are equipped with the technology (this assumes an average capacity factor of 85 percent and other assumptions of Tables 4-4 and 4-5).", "statement": "Power plants' mercury control AC usage is higher than total MWe from coal facilities.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 48, "n": 47, "e": 5}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "46059c", "context": "The results of even the most well designed epidemiological studies are characterized by this type of uncertainty, though well-designed studies typically report narrower uncertainty bounds around the best estimate than do studies of lesser quality.", "statement": "All studies have the same amount of uncertainty to them.", "label_count_round_1": {"contradiction": 3.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 61, "e": 27, "n": 12}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "82156e", "context": "The great breathtaking Italian adventure remains the road.", "statement": "The road remains the Italy people want to see.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 41, "e": 54, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "30894n", "context": "Earlier this week, the Pakistani paper Dawn ran an editorial about reports that Pakistani poppy growers are planning to recultivate opium on a bigger scale because they haven't received promised compensation for switching to other crops.", "statement": "Pakistani poppy growers are mad at the government.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 44, "n": 54, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "122645n", "context": "Then you're ready for the fray, either in the bustling great bazaars such as Delhi's Chandni Chowk or Mumbai's Bhuleshwar, or the more sedate ambience of grander shops and showrooms.", "statement": "All of the great bazaars are bustling at all times.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 21, "n": 66, "c": 13}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "26142n", "context": "The importer pays duties that are required by law", "statement": "Imported goods have duties", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": 1.0, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 9, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "66225n", "context": "uh but you could fill a whole bunch of uh holes with these things i used to i used to advertise buying wheat pennies um i'd give a dollar a roll which two cents a piece which is basically overpriced", "statement": "I made a good dollar while selling them.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 31, "e": 29, "n": 40}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"e": 1, "n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "134103n", "context": "He walked out into the street and I followed.", "statement": "I followed him down the street.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "n": 22, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "117177n", "context": "I guess history repeats itself, Jane.", "statement": "I truly think the prior situation shows history repeats itself.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 25, "e": 73, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "78105e", "context": "Their supplies scarce, their harvest meager, and their spirit broken, they abandoned the fort in 1858.", "statement": "Their supplies remained very low and hard to maintain.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 81, "n": 18, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "23280e", "context": "Sphinxes were guardian deitiesinEgyptianmythologyandthis was monumentalprotection,standing73 m (240 ft)longand20 m (66 feet) high.", "statement": "Sphinxes guarded people.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 45, "e": 52, "c": 3}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "9393n", "context": "Next, you enter the vast and splendid Imperial Hall, with three handsome marble fountains, and a canopied throne from which the sultan would enjoy the music and dancing of his concubines.", "statement": "The sultan enjoyed drinking from the marble fountains in the Imperial Hall.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 70, "e": 7, "c": 23}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "82510c", "context": "although the uh it's uh it we almost one day we painted the house to uh we painted we painted the whole inside and it had all this dark trim we thought uh you know we did the one wall but the other trim i'm trying to think i think i think we left most of it because it gets to be uh they don't do that in the newer houses now we don't the uh mold everything is white in a new house everything is white", "statement": "It took over a day to paint the house", "label_count_round_1": {"contradiction": 3.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["contradiction"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"c": 47, "e": 23, "n": 30}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "14459n", "context": "After their savage battles, the warriors recuperated through meditation in the peace of a Zen monastery rock garden.", "statement": "The warriors recuperated through mediation learned from monks.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 34, "n": 62, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "70711c", "context": "because otherwise it's too it gets if you start them when it's cooler in the spring then it gets too hot in the summer", "statement": "You should start them during Spring if you want them to be cool during the summer.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 38, "c": 40, "e": 22}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "77893n", "context": "As he stepped across the threshold, Tommy brought the picture down with terrific force on his head.", "statement": "Tommy hurt his head bringing the picture down.", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 25, "e": 72, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "20181n", "context": "What the judge really wants are the facts -- he wants to make a good decision, he said.", "statement": "In the end the judge made a bad decision since he imprisoned someone innocent.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 27, "n": 70, "e": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "137715n", "context": "We still espouse a God-given right of human beings to use the environment for their benefit, says Barrett Duke of the Southern Baptists.", "statement": "Human beings are entitled to the environment.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 81, "n": 16, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"e": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "109679n", "context": "The Palace of Jahangir is built around a square court with arches.", "statement": "The Palace of Jahangir houses a wonderful square court, complete with arches.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["entailment"], "error_labels": ["neutral"], "has_ambiguity": false, "chaosnli_labels": {"e": 86, "n": 14}, "error_llm": [], "not_validated_exp_llm": {"e": 2, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "123891c", "context": "His proud reserve--a product of 40 years in the spotlight--is refreshing but does not bode well for his capacity to shepherd big ideas through Congress.", "statement": "He is way too loud.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 56, "c": 40, "e": 4}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "16086n", "context": "Unless the report is restricted by law or regulation, auditors should ensure that copies be made available for public inspection.", "statement": "This report is most likely restricted by law or regulation and should not be ensured.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 38, "n": 52, "e": 10}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "91650n", "context": "yep because it's when it's self propelled it's heavy yeah", "statement": "it's heavy when it's self propelled, in case you were wondering", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 93, "n": 5, "c": 2}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "85428e", "context": "Christ on a crutch, what does he have to do to lose your support, stab David Geffen with a kitchen knife?", "statement": "Your support is unwavering.", "label_count_round_1": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 78, "n": 18, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"n": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "118403n", "context": "oh really it wouldn't matter if we plant them when it was starting to get warmer", "statement": "It is better to plant when it is colder.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 41, "n": 52, "e": 7}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "105561e", "context": "and they just put instructors out there and you you sign up for instruction and they just give you an arm band and if you see an instructor who's not doing anything you just tap him on the shoulder and ask him questions and they'll show you things", "statement": "The instructors are marked with armbands, and anytime you want to know anything, you just find one of them.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": ["entailment"], "has_ambiguity": true, "chaosnli_labels": {"n": 14, "e": 60, "c": 26}, "error_llm": [], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "11303n", "context": "'I see.'", "statement": "It was clear", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 67, "n": 31, "c": 2}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "11534n", "context": "He found himself thinking in circles of worry and pulled himself back to his problem.", "statement": "He could not afford to get distracted from his problem.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 51, "n": 45, "c": 4}, "error_llm": [], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "46198n", "context": "How effectively DOD manages these funds will determine whether it receives a good return on its investment.", "statement": "These funds are for the purchase of five thousand tons of potatoes.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 74, "c": 24, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "125013e", "context": "yeah okay yeah those games are fun to watch you you you watch those games", "statement": "Those games are a lot of fun.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 74, "n": 26}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "44747n", "context": "Total volume grew 13.", "statement": "The expected increase was 10.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 3, "n": 69, "c": 28}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "19768c", "context": "Wear a nicely ventilated hat and keep to the shade in the street.", "statement": "The buildings are so low that there is no shade in the streets.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 61, "n": 37, "e": 2}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 3}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "53468n", "context": "But is the Internet so miraculous an advertising vehicle that Gross will be able to siphon off $400 per person from total ad spending of $1,000 per family--or persuade advertisers to spend an additional $400 to reach each of his customers?", "statement": "The internet is so great at advertising that is saved Gross money.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"e": 36, "n": 56, "c": 8}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "136752n", "context": "The questions may need to be tailored to", "statement": "A majority of the questions referenced will need to be tailored to.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"e": 43, "n": 54, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "10724n", "context": "Traditionally, certain designs were reserved for royalty, but today elegant geometric or exuberant, stylized floral patterns are available to all.", "statement": "Designs once reserved for royalty cost more to buy.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 78, "c": 18, "e": 4}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "17753n", "context": "The street ends at Taksim Square (Taksim Meydane), the heart of modern Istanbul, lined with luxurious five-star hotels and the glass-fronted Ataturk Cultural Centre (Ataturk Keleter Sarayy), also called the Opera House.", "statement": "The street is quite a luxurious one.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 88, "n": 12}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "130928n", "context": "Still, I guess that can be got over.", "statement": "There are some things that you need to ignore.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 45, "e": 51, "c": 4}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "66858n", "context": "Managing better requires that agencies have, and rely upon, sound financial and program information.", "statement": "Agencies that rely on information based on unsound financial information will have management problems.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 75, "c": 7, "n": 18}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "28601n", "context": "Three more days went by in dreary inaction.", "statement": "The days passed by slowly.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 78, "n": 21, "c": 1}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "92774n", "context": "The party's broad aims were to support capitalist policies and to continue close ties with Britain and the rest of the Commonwealth.", "statement": "The party sought to establish ties with the United States.", "label_count_round_1": {"contradiction": 2.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 46, "c": 53, "e": 1}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "142604e", "context": "As the budgets, functions, and points of service of many government programs devolve to state and local government, private entities and nonprofit organizations, and other third parties, it may become harder for GAO to obtain the records it needs to complete audits and evaluations.", "statement": "Audits and evaluations are harder because it is more difficult for GAO to get the records.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"n": 19, "e": 81}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "42860n", "context": "That's why we tried to kill you.", "statement": "That's one of the reasons we wanted to kill you.", "label_count_round_1": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": 2.0, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["contradiction", "neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 17, "e": 75, "c": 8}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "77299n", "context": "The inquiry expanded very quickly, however, from asking what technology failed to an examination of contextual influences, such as", "statement": "They moved they inquiries over from technology failing because they thought it may be something else.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 9, "e": 46, "n": 45}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "13765c", "context": "it's just it's the morals of the people which i mean i guess we everybody's responsible for the society but if i had a child that that did things so bad it's not they don't care about anybody these people they're stealing from they're just the big bad rich guy", "statement": "I have no issue with people stealing from others.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": [], "error_labels": ["contradiction", "entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 31, "n": 60, "e": 9}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "entailment"], "label": [0.5, 0.0, 0.5]}
{"id": "139362e", "context": "Endorphins were flowing.", "statement": "My endorphins were flowing.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"n": 50, "e": 47, "c": 3}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "116176c", "context": "Students of human misery can savor its underlying sadness and futility.", "statement": "Students of human misery will be delighted to see how sad it truly is.", "label_count_round_1": {"contradiction": 1.0, "entailment": 1.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 69, "n": 26, "c": 5}, "error_llm": [], "not_validated_exp_llm": {"c": 1}, "label_set_llm": ["contradiction", "entailment", "neutral"], "label": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}
{"id": "110234e", "context": "really oh i thought it was great yeah", "statement": "that was a nice experience", "label_count_round_1": {"contradiction": 1.0, "entailment": 3.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 3.0, "neutral": 1.0}, "label_set_round_1": ["contradiction", "neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": ["contradiction"], "has_ambiguity": true, "chaosnli_labels": {"e": 81, "n": 19}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "84781c", "context": "By coordinating policy development and awareness activities in this manner, she helps ensure that new risks and policies are communicated promptly and that employees are periodically reminded of existing policies through means such as monthly bulletins, an intranet web site, and presentations to new employees.", "statement": "There new employees are a risk.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 3.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral"], "error_labels": ["contradiction"], "has_ambiguity": false, "chaosnli_labels": {"n": 54, "c": 35, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"n": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "41052n", "context": "HCFA published a Notice of Proposed Rulemaking on March 28, 1997 (62 Fed.", "statement": "HCFA tried to keep everyone informed about the rules they were making.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 2.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 31, "e": 68, "c": 1}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 3}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "128160n", "context": "Suddenly she started, and her face blanched.", "statement": "She moved swiftly, her face pale.", "label_count_round_1": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 3.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 7, "e": 78, "n": 15}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "123038e", "context": "Reports on attestation engagements should state that the engagement was made in accordance with generally accepted government auditing standards.", "statement": "Details regarding validation engagements ought to express that the engagement was made as per by and large acknowledged government evaluating guidelines.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 66, "n": 27, "c": 7}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "98621n", "context": "In other cases, we must rely on survey approaches to estimate WTP, usually through a variant of the contingent valuation approach, which generally involves directly questioning respondents for their WTP in hypothetical market situations.", "statement": "Hypothetical market situations are uniform across all respondents.", "label_count_round_1": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_count_round_2": {"contradiction": 1.0, "entailment": null, "neutral": 2.0}, "label_set_round_1": ["neutral", "contradiction"], "label_set_round_2": ["neutral", "contradiction"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"c": 39, "n": 54, "e": 7}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 1}, "label_set_llm": ["contradiction", "neutral"], "label": [0.0, 0.5, 0.5]}
{"id": "97926c", "context": "General Motors, for instance, lost $460 million to strikes in 1997, but investors treated the costs as a kind of extraordinary charge and valued the company as if the losses had never happened.", "statement": "GM lost a lot almost a million dollars in labor disputes.", "label_count_round_1": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_count_round_2": {"contradiction": 4.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction"], "label_set_round_2": ["contradiction"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 72, "n": 17, "e": 11}, "error_llm": [], "not_validated_exp_llm": {"n": 1, "c": 1}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
{"id": "98561e", "context": "was it bad", "statement": "Was it not good?", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"e": 84, "n": 16}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 2, "c": 3}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "86429n", "context": "Agencies may perform the analyses required by sections 603 and 604 in conjunction with or as part of any other agenda or analysis required by other law if such other analysis satisfies the provisions of these sections.", "statement": "There are many times when the agencies have gotten in trouble.", "label_count_round_1": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_count_round_2": {"contradiction": null, "entailment": null, "neutral": 4.0}, "label_set_round_1": ["neutral"], "label_set_round_2": ["neutral"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"n": 77, "c": 20, "e": 3}, "error_llm": ["entailment", "contradiction"], "not_validated_exp_llm": {"e": 3, "c": 2}, "label_set_llm": ["neutral"], "label": [0.0, 1.0, 0.0]}
{"id": "105911c", "context": "and to have children and just get a day care or someone to take care of it and not really have the bonding process that takes place with babies and stuff you know", "statement": "The children should not go to day car.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 1.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"e": 37, "n": 50, "c": 13}, "error_llm": [], "not_validated_exp_llm": {}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "126486n", "context": "The entire setup has an anti-competitive, anti-entrepreneurial flavor that rewards political lobbying rather than good business practices.", "statement": "The setup has lead to increases in political lobbying.", "label_count_round_1": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_count_round_2": {"contradiction": null, "entailment": 2.0, "neutral": 1.0}, "label_set_round_1": ["neutral", "entailment"], "label_set_round_2": ["neutral", "entailment"], "error_labels": [], "has_ambiguity": true, "chaosnli_labels": {"n": 33, "e": 67}, "error_llm": ["contradiction"], "not_validated_exp_llm": {"n": 1, "c": 2}, "label_set_llm": ["entailment", "neutral"], "label": [0.5, 0.5, 0.0]}
{"id": "127809n", "context": "I'm confused.", "statement": "Not all of it is very clear to me.", "label_count_round_1": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_count_round_2": {"contradiction": null, "entailment": 4.0, "neutral": null}, "label_set_round_1": ["entailment"], "label_set_round_2": ["entailment"], "error_labels": [], "has_ambiguity": false, "chaosnli_labels": {"c": 3, "e": 92, "n": 5}, "error_llm": ["neutral", "contradiction"], "not_validated_exp_llm": {"n": 3, "c": 2}, "label_set_llm": ["entailment"], "label": [1.0, 0.0, 0.0]}
{"id": "28306c", "context": "They made little effort, despite the Jesuit presence in Asia, to convert local inhabitants to Christianity or to expand their territory into the interior.", "statement": "The Jesuit presence in Asia helped to convert local residents to Christianity, allowing them to expand their territory.", "label_count_round_1": {"contradiction": 4.0, "entailment": 1.0, "neutral": null}, "label_count_round_2": {"contradiction": 3.0, "entailment": null, "neutral": null}, "label_set_round_1": ["contradiction", "entailment"], "label_set_round_2": ["contradiction"], "error_labels": ["entailment"], "has_ambiguity": false, "chaosnli_labels": {"c": 69, "e": 14, "n": 17}, "error_llm": ["entailment"], "not_validated_exp_llm": {"e": 2}, "label_set_llm": ["contradiction"], "label": [0.0, 0.0, 1.0]}
