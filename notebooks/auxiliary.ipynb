{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add id to original label guided dataset\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "file_a_path = \"../VariErr-Label-Guided-longest.json\"\n",
    "file_b_path = \"../varierr.json\"\n",
    "output_path = \"../VariErr-Label-Guided-longest-with-ID.json\"\n",
    "\n",
    "with open(file_b_path, \"r\") as f:\n",
    "    full_dataset = [json.loads(line) for line in f]\n",
    "    pair_to_id = {\n",
    "        (sample[\"context\"].strip(), sample[\"statement\"].strip()): sample[\"id\"]\n",
    "        for sample in full_dataset\n",
    "    }\n",
    "\n",
    "print(f\"We have {len(pair_to_id)} pairs.\")\n",
    "\n",
    "with open(file_a_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_path, \"w\") as f_out:\n",
    "    for sample in tqdm(data):\n",
    "        premise = sample[\"premise\"].strip()\n",
    "        hypothesis = sample[\"hypothesis\"].strip()\n",
    "        key = (premise, hypothesis)\n",
    "\n",
    "        if key in pair_to_id:\n",
    "            sample[\"id\"] = pair_to_id[key]\n",
    "        else:\n",
    "            print(f\"Not found: premise='{premise}...', hypothesis='{hypothesis}...'\")\n",
    "            sample[\"id\"] = None\n",
    "\n",
    "        f_out.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inject explanations: 100%|██████████| 500/500 [00:00<00:00, 1360.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# integrate explanations generated by LLMs to a singel file\n",
    "\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "explanation_root = Path(\"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\")\n",
    "input_jsonl = Path(\"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/dataset/varierr/varierr.json\")\n",
    "output_jsonl = Path(\"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw.jsonl\")\n",
    "\n",
    "# suffix = \".txt\"\n",
    "\n",
    "def clean_explanation(text: str) -> str:\n",
    "    return re.sub(r\"^\\s*(?:[\\d]+[\\.\\)]|[-•*]|[a-zA-Z][\\.\\)]|\\(\\w+\\))\\s+\", \"\", text).strip()\n",
    "\n",
    "label_map = {\"E\": \"e\", \"N\": \"n\", \"C\": \"c\"}\n",
    "\n",
    "with open(input_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "    instances = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for instance in tqdm(instances, desc=\"Inject explanations\"):\n",
    "        sample_id = str(instance[\"id\"])\n",
    "        subfolder = explanation_root / sample_id\n",
    "        new_comments = []\n",
    "\n",
    "        if not subfolder.exists():\n",
    "            print(f\"missing folder: {subfolder}\")\n",
    "        else:\n",
    "            for label in [\"E\", \"N\", \"C\"]:\n",
    "                tried_files = [\n",
    "                    # f\"{label}_third.txt\"\n",
    "                    # f\"{label}_second.txt\",\n",
    "                    # f\"{label}_first.txt\",\n",
    "                    label\n",
    "                ]\n",
    "        \n",
    "                file_found = False\n",
    "                for fname in tried_files:\n",
    "                    file_path = subfolder / f\"{fname}\"\n",
    "                    if file_path.exists():\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            explanations = [\n",
    "                                clean_explanation(line)\n",
    "                                for line in f\n",
    "                                if line.strip()\n",
    "                            ]\n",
    "                        new_comments.extend([[exp, label_map[label]] for exp in explanations])\n",
    "                        file_found = True\n",
    "                        break\n",
    "\n",
    "                if not file_found:\n",
    "                    print(f\"No file found for {label} in {subfolder}\")\n",
    "        new_instance = {\n",
    "            \"id\": instance[\"id\"],\n",
    "            \"premise\": instance[\"context\"],\n",
    "            \"hypothesis\": instance[\"statement\"],\n",
    "            \"generated_explanations\": new_comments\n",
    "        }\n",
    "\n",
    "        fout.write(json.dumps(new_instance, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E=1198，N=1407，C=1415，total: 4020\n"
     ]
    }
   ],
   "source": [
    "# count number\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_FOLDER = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\"\n",
    "\n",
    "def count_generations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for line in f if line.strip())\n",
    "\n",
    "def count_all_generations():\n",
    "    records = []\n",
    "    total = {\"E\": 0, \"N\": 0, \"C\": 0}\n",
    "\n",
    "    for subfolder in os.listdir(ROOT_FOLDER):\n",
    "        sub_path = os.path.join(ROOT_FOLDER, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "\n",
    "        row = {\"folder\": subfolder}\n",
    "        for label in [\"E\", \"N\", \"C\"]:\n",
    "            file_path = os.path.join(sub_path, f\"{label}_third.txt\")\n",
    "            if os.path.isfile(file_path):\n",
    "                count = count_generations(file_path)\n",
    "            else:\n",
    "                count = 0\n",
    "            row[label] = count\n",
    "            total[label] += count\n",
    "\n",
    "        records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.loc[\"TOTAL\"] = [\"TOTAL\"] + [total[\"E\"], total[\"N\"], total[\"C\"]]\n",
    "    # print(df)\n",
    "\n",
    "    print(f\"E={total['E']}，N={total['N']}，C={total['C']}，total: {total['E'] + total['N'] + total['C']}\")\n",
    "\n",
    "count_all_generations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get avg score for instance\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('../scores.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for key, value in data.items():\n",
    "    try:\n",
    "        id_label, _ = key.rsplit('-', 1)\n",
    "        groups[id_label].append(value)\n",
    "    except ValueError:\n",
    "        print(f\"'{key}' does not match.\")\n",
    "        continue\n",
    "\n",
    "averaged_data = {k: sum(v) / len(v) for k, v in groups.items()}\n",
    "with open('../avg_llama3.1_scores.jsonn', 'w') as f:\n",
    "    json.dump(averaged_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thresholding for ChaosNLI\n",
    "import json\n",
    "\n",
    "def process_label_distribution(label_probs, threshold=0.2):\n",
    "    valid_indices = [i for i, p in enumerate(label_probs) if p >= threshold]\n",
    "    count = len(valid_indices)\n",
    "    if count == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    return [1.0 / count if i in valid_indices else 0.0 for i in range(3)]\n",
    "\n",
    "input_file = '../dev_cleaned.json'\n",
    "output_file = '../dev_cleaned_20.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        item = json.loads(line)\n",
    "        raw_label = item['label']\n",
    "        new_label = process_label_distribution(raw_label)\n",
    "        item['label'] = new_label\n",
    "        fout.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第二列 llm_not_calidated_error：\n",
      "  e: 76  n: 69  c: 247\n",
      "第三列 varierr_error：\n",
      "  e: 53  n: 23  c: 53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "path = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/llama-70b-all/merged_errors.csv\"  # 改成你的文件路径\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 把单元格里的 '[\"c\"]'、'[]' 等转为 Python 列表；兼容引号异常\n",
    "def parse_list_cell(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        # 某些 CSV 里会出现双引号翻倍的情况\n",
    "        try:\n",
    "            return ast.literal_eval(s.replace('\"\"', '\"'))\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# 第三列映射为 e/n/c\n",
    "map_long = {\"entailment\": \"e\", \"neutral\": \"n\", \"contradiction\": \"c\",\n",
    "            \"e\": \"e\", \"n\": \"n\", \"c\": \"c\"}\n",
    "\n",
    "cnt2 = Counter()\n",
    "cnt3 = Counter()\n",
    "\n",
    "# 统计第二列（通常是 'e'/'n'/'c'）\n",
    "for items in df[\"llm_not_calidated_error\"].apply(parse_list_cell):\n",
    "    for it in items:\n",
    "        k = map_long.get(it, None)\n",
    "        if k in (\"e\", \"n\", \"c\"):\n",
    "            cnt2[k] += 1\n",
    "\n",
    "# 统计第三列（可能是完整单词）\n",
    "for items in df[\"varierr_error\"].apply(parse_list_cell):\n",
    "    for it in items:\n",
    "        k = map_long.get(it, None)\n",
    "        if k in (\"e\", \"n\", \"c\"):\n",
    "            cnt3[k] += 1\n",
    "\n",
    "print(\"第二列 llm_not_calidated_error：\")\n",
    "print(f\"  e: {cnt2.get('e',0)}  n: {cnt2.get('n',0)}  c: {cnt2.get('c',0)}\")\n",
    "\n",
    "print(\"第三列 varierr_error：\")\n",
    "print(f\"  e: {cnt3.get('e',0)}  n: {cnt3.get('n',0)}  c: {cnt3.get('c',0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 10 | id=48454c\n",
      "  重叠标签个数：1\n",
      "Row 27 | id=16996e\n",
      "  重叠标签个数：1\n",
      "Row 60 | id=65130n\n",
      "  重叠标签个数：1\n",
      "Row 84 | id=49462c\n",
      "  重叠标签个数：1\n",
      "Row 98 | id=52542n\n",
      "  重叠标签个数：1\n",
      "Row 115 | id=82415n\n",
      "  重叠标签个数：1\n",
      "Row 138 | id=80630e\n",
      "  重叠标签个数：1\n",
      "Row 152 | id=106013c\n",
      "  重叠标签个数：1\n",
      "Row 176 | id=89995c\n",
      "  重叠标签个数：1\n",
      "Row 186 | id=17576n\n",
      "  重叠标签个数：1\n",
      "Row 188 | id=64123c\n",
      "  重叠标签个数：1\n",
      "Row 206 | id=84055n\n",
      "  重叠标签个数：1\n",
      "Row 234 | id=124853e\n",
      "  重叠标签个数：1\n",
      "Row 241 | id=87332c\n",
      "  重叠标签个数：1\n",
      "Row 260 | id=73191n\n",
      "  重叠标签个数：1\n",
      "Row 289 | id=75259c\n",
      "  重叠标签个数：1\n",
      "Row 300 | id=102857n\n",
      "  重叠标签个数：1\n",
      "Row 303 | id=104412e\n",
      "  重叠标签个数：1\n",
      "Row 311 | id=21340n\n",
      "  重叠标签个数：1\n",
      "Row 319 | id=34043c\n",
      "  重叠标签个数：1\n",
      "Row 358 | id=32754n\n",
      "  重叠标签个数：1\n",
      "Row 366 | id=34573n\n",
      "  重叠标签个数：1\n",
      "Row 369 | id=118460n\n",
      "  重叠标签个数：1\n",
      "Row 379 | id=138530e\n",
      "  重叠标签个数：1\n",
      "Row 394 | id=76037n\n",
      "  重叠标签个数：1\n",
      "Row 431 | id=81579e\n",
      "  重叠标签个数：1\n",
      "Row 443 | id=46059c\n",
      "  重叠标签个数：1\n",
      "Row 454 | id=82510c\n",
      "  重叠标签个数：1\n",
      "Row 481 | id=142604e\n",
      "  重叠标签个数：1\n",
      "Row 484 | id=13765c\n",
      "  重叠标签个数：1\n",
      "Row 486 | id=116176c\n",
      "  重叠标签个数：1\n",
      "Row 487 | id=110234e\n",
      "  重叠标签个数：1\n",
      "Row 499 | id=28306c\n",
      "  重叠标签个数：1\n",
      "\n",
      "===== 汇总 =====\n",
      "存在两列重叠标签的行数：33 / 500\n",
      "重叠标签总计（按 e/n/c）：\n",
      "  e: 8\n",
      "  n: 1\n",
      "  c: 24\n",
      "每行重叠个数分布：\n",
      "  重叠 1 个标签的行数：33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "path = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/llama-70b-all/merged_errors.csv\"  # 改成你的 CSV 路径\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def parse_list_cell(x):\n",
    "    \"\"\"把 '[\"c\"]' / '[]' / 空 转成 Python 列表\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        # 修正可能的双引号转义\n",
    "        try:\n",
    "            return ast.literal_eval(s.replace('\"\"', '\"'))\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# 统一成 e/n/c\n",
    "MAP = {\"entailment\": \"e\", \"neutral\": \"n\", \"contradiction\": \"c\",\n",
    "       \"e\": \"e\", \"n\": \"n\", \"c\": \"c\"}\n",
    "\n",
    "def normalize(lst):\n",
    "    out = []\n",
    "    for it in lst:\n",
    "        k = MAP.get(it, None)\n",
    "        if k in (\"e\",\"n\",\"c\"):\n",
    "            out.append(k)\n",
    "    return out\n",
    "\n",
    "def dupes(lst):\n",
    "    c = Counter(lst)\n",
    "    return [k for k, v in c.items() if v > 1]\n",
    "\n",
    "overlap_rows = 0\n",
    "overlap_label_counter = Counter()        # 统计重叠 label 总数（按 e/n/c）\n",
    "overlap_per_row_counter = Counter()      # 每行重叠个数分布（1/2/3）\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    rid = row.get(\"id\", idx)  # 若无 id 列则用行号\n",
    "    col2_raw = parse_list_cell(row[\"llm_not_calidated_error\"])\n",
    "    col3_raw = parse_list_cell(row[\"varierr_error\"])\n",
    "\n",
    "    col2 = normalize(col2_raw)\n",
    "    col3 = normalize(col3_raw)\n",
    "\n",
    "    # 列内重复（可选，如果你只关心两列之间重叠，这两行可以删掉）\n",
    "    d2 = dupes(col2)\n",
    "    d3 = dupes(col3)\n",
    "\n",
    "    # 两列之间的重叠（按集合计算）\n",
    "    overlap = sorted(set(col2) & set(col3))\n",
    "\n",
    "    if d2 or d3 or overlap:\n",
    "        print(f\"Row {idx} | id={rid}\")\n",
    "        if d2:\n",
    "            print(f\"  重复(第二列 llm_not_calidated_error): {d2}  原始: {col2_raw}\")\n",
    "        if d3:\n",
    "            print(f\"  重复(第三列 varierr_error): {d3}       原始: {col3_raw}\")\n",
    "        if overlap:\n",
    "            # print(f\"  两列之间有重叠: {overlap}  第二列: {col2}  第三列: {col3}\")\n",
    "            print(f\"  重叠标签个数：{len(overlap)}\")\n",
    "\n",
    "    if overlap:\n",
    "        overlap_rows += 1\n",
    "        overlap_label_counter.update(overlap)          # 统计 e/n/c\n",
    "        overlap_per_row_counter[len(overlap)] += 1     # 记录每行重叠了几个标签\n",
    "\n",
    "# ---- 汇总统计 ----\n",
    "total_rows = len(df)\n",
    "print(\"\\n===== 汇总 =====\")\n",
    "print(f\"存在两列重叠标签的行数：{overlap_rows} / {total_rows}\")\n",
    "\n",
    "print(\"重叠标签总计（按 e/n/c）：\")\n",
    "for k in (\"e\",\"n\",\"c\"):\n",
    "    print(f\"  {k}: {overlap_label_counter.get(k, 0)}\")\n",
    "\n",
    "print(\"每行重叠个数分布：\")\n",
    "for n in sorted(overlap_per_row_counter):\n",
    "    print(f\"  重叠 {n} 个标签的行数：{overlap_per_row_counter[n]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine multiple generations by ID    \n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "REQ_FIELDS = [\"id\", \"premise\", \"hypothesis\", \"generated_explanations\"]\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def merge_by_id(files):\n",
    "    merged = OrderedDict() \n",
    "    for fp in files:\n",
    "        for obj in read_jsonl(fp):\n",
    "            if not all(k in obj for k in REQ_FIELDS):\n",
    "                continue\n",
    "\n",
    "            _id = obj[\"id\"]\n",
    "            if _id not in merged:\n",
    "                merged[_id] = {\n",
    "                    \"id\": _id,\n",
    "                    \"premise\": obj[\"premise\"],\n",
    "                    \"hypothesis\": obj[\"hypothesis\"],\n",
    "                    \"generated_explanations\": []\n",
    "                }\n",
    "                existing = set()\n",
    "            else:\n",
    "                # 如有不一致，保留第一次出现的 premise/hypothesis\n",
    "                existing = {\n",
    "                    (e[0], e[1]) if isinstance(e, list) and len(e) >= 2 else tuple(e)\n",
    "                    for e in merged[_id][\"generated_explanations\"]\n",
    "                }\n",
    "\n",
    "            # 合并 explanations 并去重（基于 (text,label)）\n",
    "            # for e in obj.get(\"generated_explanations\", []):\n",
    "            #     key = (e[0], e[1]) if isinstance(e, list) and len(e) >= 2 else tuple(e)\n",
    "            #     if key not in existing:\n",
    "            #         merged[_id][\"generated_explanations\"].append(e)\n",
    "            #         existing.add(key)\n",
    "    return merged\n",
    "\n",
    "def write_jsonl(merged, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in merged.values():\n",
    "            json.dump(rec, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"合并多个 JSONL（绝对路径），按 id 合并 generated_explanations。\")\n",
    "    parser.add_argument(\"files\", nargs=\"+\", help=\"一个或多个 JSONL 文件的绝对路径\")\n",
    "    parser.add_argument(\"-o\", \"--output\", default=\"generation_all.jsonl\", help=\"输出文件路径\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    merged = merge_by_id(args.files)\n",
    "    write_jsonl(merged, args.output)\n",
    "    print(f\"完成：合并 {len(merged)} 个 id，已保存到 {args.output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
