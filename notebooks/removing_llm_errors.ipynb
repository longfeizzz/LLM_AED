{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# remove LLM-detected errors from original VariErr\n",
    "\n",
    "import json\n",
    "\n",
    "varierr_file = '/Users/phoebeeeee/ongoing/LLM_AED/dataset/varierr/varierr.json'\n",
    "model_file = '/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/with_validation_0.8.jsonl'\n",
    "output_file = '/Users/phoebeeeee/ongoing/LLM_AED/dataset/varierr/varierr_original.json'\n",
    "\n",
    "with open(varierr_file, 'r', encoding='utf-8') as f:\n",
    "    varierr_data = {json.loads(line)['id']: json.loads(line) for line in f}\n",
    "\n",
    "with open(model_file, 'r', encoding='utf-8') as f:\n",
    "    model_data = {json.loads(line)['id']: json.loads(line) for line in f}\n",
    "label_map = {'e': 'entailment', 'n': 'neutral', 'c': 'contradiction'}\n",
    "\n",
    "merged = []\n",
    "\n",
    "for uid, var_entry in varierr_data.items():\n",
    "    model_entry = model_data.get(uid, {})\n",
    "\n",
    "    var_entry.pop('entailment', None)\n",
    "    var_entry.pop('contradiction', None)\n",
    "    var_entry.pop('neutral', None)\n",
    "    var_entry.pop('idk', None)\n",
    "\n",
    "    if 'error' in model_entry:\n",
    "        error_raw = model_entry['error']\n",
    "        error_mapped = [label_map.get(lbl, lbl) for lbl in error_raw]\n",
    "        var_entry['error_llm'] = error_mapped\n",
    "    if 'not_validated_exp' in model_entry:\n",
    "        var_entry['not_validated_exp_llm'] = model_entry['not_validated_exp']\n",
    "\n",
    "    original_labels = set(var_entry.get('label_set_round_1', []))\n",
    "    error_labels = set(var_entry.get('error_llm', []))\n",
    "    label_set_llm = sorted(original_labels - error_labels)\n",
    "\n",
    "    var_entry['label_set_llm'] = label_set_llm\n",
    "    merged.append(var_entry)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out:\n",
    "    for item in merged:\n",
    "        json.dump(item, out, ensure_ascii=False)\n",
    "        out.write('\\n')\n",
    "\n",
    "print(f\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44fa03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "input_path = \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/without_llm_error/varierr_without_0.8.json\"\n",
    "output_path = \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/without_llm_error/varierr_without_0.8_dist.json\"\n",
    "\n",
    "def convert_label_list_to_dist(label_list):\n",
    "    counter = Counter(label_list or [])\n",
    "    e = counter.get(\"entailment\", 0)\n",
    "    n = counter.get(\"neutral\", 0)\n",
    "    c = counter.get(\"contradiction\", 0)\n",
    "\n",
    "    total = e + n + c\n",
    "    if total == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    return [e / total, n / total, c / total] \n",
    "\n",
    "with open(input_path, \"r\") as infile, \\\n",
    "     open(output_path, \"w\") as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        item = json.loads(line)\n",
    "\n",
    "        label_list = item.get(\"label_set_llm\")\n",
    "        if label_list:\n",
    "            item[\"label\"] = convert_label_list_to_dist(label_list)\n",
    "        else:\n",
    "            item[\"label\"] = [0.0, 0.0, 0.0]\n",
    "        json.dump(item, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b84cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/without_llm_error/varierr_without_0.8_dist.json: 500it [00:00, 102480.06it/s]\n"
     ]
    }
   ],
   "source": [
    "## clean\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_label_dist(chaos_dict):\n",
    "   \n",
    "    label_order = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    values = [(chaos_dict.get(k) or 0.0) for k in label_order] \n",
    "    total = sum(values)\n",
    "    if total == 0:\n",
    "        return [0.0] * 3\n",
    "    return [v / total for v in values]\n",
    "\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in tqdm(fin, desc=f\"Processing {input_path}\"):\n",
    "            raw = json.loads(line)\n",
    "            out = {\n",
    "                \"uid\": raw.get(\"id\", raw.get(\"uid\")),\n",
    "                \"premise\": raw.get(\"context\"),\n",
    "                \"hypothesis\": raw.get(\"statement\"),\n",
    "                \"label\": raw.get(\"label\")\n",
    "            }\n",
    "            fout.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path =  \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/without_llm_error/varierr_without_0.8_dist.json\"\n",
    "    output_path = \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/without_llm_error/varierr_without_0.8_cleaned.json\"\n",
    "    process_file(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b84dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Processing threshold 0.1\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.1_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.2\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.2_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.3\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.3_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.4\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.4_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.5\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.5_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.6\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.6_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.7\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.7_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.8\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.8_cleaned.json\n",
      "\n",
      "==> Processing threshold 0.9\n",
      "[OK] Saved cleaned: /Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error/varierr_without_0.9_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 固定路径\n",
    "VARIERR_FILE = \"/Users/phoebeeeee/ongoing/LLM_AED/dataset/varierr/varierr.json\"\n",
    "MODEL_DIR = \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold\"\n",
    "OUT_DIR = \"/Users/phoebeeeee/ongoing/LLM_AED/new_processing/validation_result/all/llama_8b_all/threshold/without_llm_error\"\n",
    "\n",
    "THRESHOLDS = [f\"{x/10:.1f}\" for x in range(1, 10)]  # 0.1 ~ 0.9\n",
    "\n",
    "label_map_short2long = {'e': 'entailment', 'n': 'neutral', 'c': 'contradiction'}\n",
    "label_order = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "def load_jsonl_to_dict_by_id(path: Path):\n",
    "    data = {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            data[obj[\"id\"]] = obj\n",
    "    return data\n",
    "\n",
    "def convert_label_list_to_dist(label_list):\n",
    "    counter = Counter(label_list or [])\n",
    "    e = counter.get(\"entailment\", 0)\n",
    "    n = counter.get(\"neutral\", 0)\n",
    "    c = counter.get(\"contradiction\", 0)\n",
    "    total = e + n + c\n",
    "    if total == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    return [e / total, n / total, c / total]\n",
    "\n",
    "def make_clean_record(raw):\n",
    "    return {\n",
    "        \"uid\": raw.get(\"id\", raw.get(\"uid\")),\n",
    "        \"premise\": raw.get(\"context\"),\n",
    "        \"hypothesis\": raw.get(\"statement\"),\n",
    "        \"label\": raw.get(\"label\"),\n",
    "    }\n",
    "\n",
    "def process_one_threshold(thr_str: str):\n",
    "    model_file = Path(MODEL_DIR) / f\"with_validation_{thr_str}.jsonl\"\n",
    "    out_file_cleaned = Path(OUT_DIR) / f\"varierr_without_{thr_str}_cleaned.json\"\n",
    "\n",
    "    if not model_file.exists():\n",
    "        print(f\"[WARN] Model file not found for threshold {thr_str}: {model_file}\")\n",
    "        return\n",
    "\n",
    "    # 1) 读取 varierr 与 model\n",
    "    print(f\"\\n==> Processing threshold {thr_str}\")\n",
    "    varierr_path = Path(VARIERR_FILE)\n",
    "    model_path = Path(model_file)\n",
    "    out_file_cleaned.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    varierr_data = load_jsonl_to_dict_by_id(varierr_path)\n",
    "    model_data = load_jsonl_to_dict_by_id(model_path)\n",
    "\n",
    "    merged_cleaned = []\n",
    "\n",
    "    # 2) 合并 & 去除模型检测到的错误\n",
    "    for uid, var_entry in varierr_data.items():\n",
    "        # 浅拷贝，避免就地修改原字典带来副作用（也可以深拷）\n",
    "        var_entry = dict(var_entry)\n",
    "\n",
    "        model_entry = model_data.get(uid, {})\n",
    "\n",
    "        var_entry.pop('entailment', None)\n",
    "        var_entry.pop('contradiction', None)\n",
    "        var_entry.pop('neutral', None)\n",
    "        var_entry.pop('idk', None)\n",
    "\n",
    "        # 附带模型的错误标签（短 -> 长）\n",
    "        if 'error' in model_entry:\n",
    "            error_raw = model_entry['error'] or []\n",
    "            error_mapped = [label_map_short2long.get(lbl, lbl) for lbl in error_raw]\n",
    "            var_entry['error_llm'] = error_mapped\n",
    "\n",
    "        # 附带未验证解释（可选）\n",
    "        if 'not_validated_exp' in model_entry:\n",
    "            var_entry['not_validated_exp_llm'] = model_entry.get('not_validated_exp', {})\n",
    "\n",
    "        # 计算剔除错误后的标签集合\n",
    "        original_labels = set(var_entry.get('label_set_round_1', []))\n",
    "        error_labels = set(var_entry.get('error_llm', []))\n",
    "        label_set_llm = sorted(original_labels - error_labels)\n",
    "        var_entry['label_set_llm'] = label_set_llm\n",
    "\n",
    "        # 3) 将剩余标签转成分布\n",
    "        var_entry[\"label\"] = convert_label_list_to_dist(label_set_llm)\n",
    "\n",
    "        # 4) 清洗成最简记录\n",
    "        cleaned = make_clean_record(var_entry)\n",
    "        merged_cleaned.append(cleaned)\n",
    "\n",
    "    # 写 cleaned 输出\n",
    "    with out_file_cleaned.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for item in merged_cleaned:\n",
    "            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] Saved cleaned: {out_file_cleaned}\")\n",
    "\n",
    "def main():\n",
    "    for thr in THRESHOLDS:\n",
    "        process_one_threshold(thr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
