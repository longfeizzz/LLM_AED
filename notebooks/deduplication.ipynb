{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff092ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存：/Users/phoebeeeee/ongoing/LLM_AED/generation/llama_70b_generation_raw/manual_check.csv（共 4039 行）\n"
     ]
    }
   ],
   "source": [
    "# 0. Fallback Responses Removal\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"/Users/phoebeeeee/ongoing/LLM_AED/generation/llama_70b_generation_raw\")\n",
    "OUTPUT_CSV = BASE_DIR / \"manual_check.csv\"\n",
    "FILE_TYPES = [\"E\", \"N\", \"C\"]\n",
    "VALIDATION = True \n",
    "STRIP_NUMBERING = True \n",
    "\n",
    "def read_file_lines(file_path: Path, strip_numbering: bool = True):\n",
    "    lines = []\n",
    "    if not file_path.is_file():\n",
    "        return lines\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            s = raw.strip()\n",
    "            if not s:\n",
    "                continue  # 跳过空行\n",
    "            if strip_numbering:\n",
    "                s = re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", s)\n",
    "            lines.append(s)\n",
    "    return lines\n",
    "\n",
    "def aggregate_to_csv(base_dir: Path, output_csv: Path):\n",
    "    records = []\n",
    "    for sub in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "        subfolder_name = sub.name\n",
    "        for ft in FILE_TYPES:\n",
    "            fpath = sub / ft  # 期望文件名就是 'E' / 'N' / 'C'\n",
    "            if not fpath.exists():\n",
    "                continue\n",
    "            lines = read_file_lines(fpath, strip_numbering=STRIP_NUMBERING)\n",
    "            for idx, content in enumerate(lines, start=1):\n",
    "                records.append({\n",
    "                    \"validation\": VALIDATION,\n",
    "                    \"subfolder\": subfolder_name,\n",
    "                    \"file_type\": ft,\n",
    "                    \"line_no\": idx,\n",
    "                    \"content\": content\n",
    "                })\n",
    "\n",
    "    if not records:\n",
    "        print(\"未收集到任何记录，请检查目录与文件命名是否正确。\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame.from_records(\n",
    "        records,\n",
    "        columns=[\"validation\", \"subfolder\", \"file_type\", \"line_no\", \"content\"]\n",
    "    )\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"已保存：{output_csv}（共 {len(df)} 行）\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aggregate_to_csv(BASE_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4965c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成：写出 1500 个 *_0.txt 文件，删除 69 行。\n"
     ]
    }
   ],
   "source": [
    "# 0\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\")\n",
    "INPUT_CSV = BASE_DIR / \"0_exp_all.csv\"   # 改成你实际的标注结果 CSV 路径\n",
    "FILE_TYPES = [\"E\", \"N\", \"C\"]\n",
    "RENUMBER_ON_WRITE = True  \n",
    "\n",
    "def is_false_mark(v) -> bool:\n",
    "    if isinstance(v, bool):\n",
    "        return v is False\n",
    "    if pd.isna(v):\n",
    "        return False\n",
    "    s = str(v).strip().lower()\n",
    "    return s == \"false\" or s == \"0\" or s == \"FALSE\"\n",
    "\n",
    "def read_logical_lines(file_path: Path):\n",
    "\n",
    "    lines = []\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            s = raw.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            s = re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", s)\n",
    "            lines.append(s)\n",
    "    return lines\n",
    "\n",
    "def write_numbered_lines(file_path: Path, lines):\n",
    "    with file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, content in enumerate(lines, start=1):\n",
    "            f.write(f\"{i}. {content}\\n\")\n",
    "\n",
    "def main():\n",
    "    if not INPUT_CSV.is_file():\n",
    "        raise FileNotFoundError(f\"找不到标注 CSV：{INPUT_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(INPUT_CSV, dtype={\"subfolder\": str, \"file_type\": str, \"line_no\": int})\n",
    "\n",
    "    df = df[[\"validation\", \"subfolder\", \"file_type\", \"line_no\"]].copy()\n",
    "    df[\"file_type\"] = df[\"file_type\"].str.upper().str.strip()\n",
    "    df[\"subfolder\"] = df[\"subfolder\"].astype(str).str.strip()\n",
    "\n",
    "    remove_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        if is_false_mark(row[\"validation\"]):\n",
    "            key = (row[\"subfolder\"], row[\"file_type\"])\n",
    "            remove_map.setdefault(key, set()).add(int(row[\"line_no\"]))\n",
    "\n",
    "    total_deleted = 0\n",
    "    total_written = 0\n",
    "\n",
    "    for sub in sorted([p for p in BASE_DIR.iterdir() if p.is_dir()]):\n",
    "        sub_name = sub.name\n",
    "        for ft in FILE_TYPES:\n",
    "            src = sub / ft  \n",
    "            if not src.exists():\n",
    "                continue\n",
    "\n",
    "            dst = sub / f\"{ft}_0.txt\"\n",
    "            key = (sub_name, ft)\n",
    "            to_remove = remove_map.get(key, set())\n",
    "\n",
    "            if not to_remove:\n",
    "                shutil.copyfile(src, dst)\n",
    "                total_written += 1\n",
    "                continue\n",
    "\n",
    "            lines = read_logical_lines(src)\n",
    "            max_idx = len(lines)\n",
    "\n",
    "\n",
    "            valid_remove = {i for i in to_remove if 1 <= i <= max_idx}\n",
    "            if len(valid_remove) < len(to_remove):\n",
    "                bad = sorted(to_remove - valid_remove)\n",
    "                print(f\"[警告] {sub_name}/{ft} 有越界行号被忽略：{bad}（总行数={max_idx}）\")\n",
    "\n",
    "            filtered = [s for i, s in enumerate(lines, start=1) if i not in valid_remove]\n",
    "            total_deleted += len(valid_remove)\n",
    "\n",
    "            if RENUMBER_ON_WRITE:\n",
    "                write_numbered_lines(dst, filtered)\n",
    "            else:\n",
    "                with dst.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                    for s in filtered:\n",
    "                        f.write(s + \"\\n\")\n",
    "\n",
    "            total_written += 1\n",
    "\n",
    "    print(f\"完成：写出 {total_written} 个 *_0.txt 文件，删除 {total_deleted} 行。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf91b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all files and saved summaries.\n"
     ]
    }
   ],
   "source": [
    "# 1. word n-gram\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_FOLDER = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\"\n",
    "THRESHOLD = 0.5\n",
    "N_GRAMS = [1, 2, 3]\n",
    "\n",
    "def extract_explanations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub(r\"^\\d+\\.\\s*\", \"\", line.strip()) for line in lines if line.strip()]\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    tokens = text.lower().split()\n",
    "    return set(zip(*[tokens[i:] for i in range(n)])) if len(tokens) >= n else set()\n",
    "\n",
    "def compute_lexical_diversity(a, b, n):\n",
    "    a_ngrams = get_ngrams(a, n)\n",
    "    b_ngrams = get_ngrams(b, n)\n",
    "    union = a_ngrams | b_ngrams\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    intersection = a_ngrams & b_ngrams\n",
    "    diversity = 1 - len(intersection) / len(union)\n",
    "    return diversity\n",
    "\n",
    "def remove_low_lexical_diversity(explanations, n_gram_list, threshold):\n",
    "    kept = []\n",
    "    kept_indices = []\n",
    "    removed_records = []\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        is_duplicate = False\n",
    "        for j, kept_exp in enumerate(kept):\n",
    "            diversities = {\n",
    "                n: compute_lexical_diversity(exp, kept_exp, n) for n in n_gram_list\n",
    "            }\n",
    "            if all(d < threshold for d in diversities.values()):\n",
    "                removed_records.append({\n",
    "                    \"removed_index\": i + 1,\n",
    "                    \"removed_text\": exp,\n",
    "                    \"compared_to_index\": kept_indices[j] + 1,\n",
    "                    \"compared_to_text\": kept_exp,\n",
    "                    **{f\"diversity_{n}gram\": diversities[n] for n in n_gram_list},\n",
    "                    \"is_removed\": True\n",
    "                })\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            kept.append(exp)\n",
    "            kept_indices.append(i)\n",
    "            removed_records.append({\n",
    "                \"removed_index\": i + 1,\n",
    "                \"removed_text\": exp,\n",
    "                \"compared_to_index\": None,\n",
    "                \"compared_to_text\": None,\n",
    "                **{f\"diversity_{n}gram\": None for n in n_gram_list},\n",
    "                \"is_removed\": False\n",
    "            })\n",
    "    return kept, removed_records\n",
    "\n",
    "def process_all():\n",
    "    summary = []\n",
    "    all_removed_records = []\n",
    "\n",
    "    for subfolder in os.listdir(ROOT_FOLDER):\n",
    "        sub_path = os.path.join(ROOT_FOLDER, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "        for file_name in [\"E_0.txt\", \"N_0.txt\", \"C_0.txt\"]:\n",
    "            file_path = os.path.join(sub_path, file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            explanations = extract_explanations(file_path)\n",
    "\n",
    "            base = file_name.split(\"_\", 1)[0]\n",
    "            output_file = os.path.join(sub_path, f\"{base}_first.txt\")\n",
    "\n",
    "            if len(explanations) < 2:\n",
    "                shutil.copyfile(file_path, output_file)\n",
    "\n",
    "                summary.append({\n",
    "                    \"folder\": subfolder,\n",
    "                    \"file\": file_name,\n",
    "                    \"original_count\": len(explanations),\n",
    "                    \"retained_count\": len(explanations),\n",
    "                    \"reduction_rate\": 0.0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            filtered, removed_records = remove_low_lexical_diversity(\n",
    "                explanations, N_GRAMS, THRESHOLD\n",
    "            )\n",
    "\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for idx, line in enumerate(filtered, 1):\n",
    "                    f.write(f\"{idx}. {line}\\n\")\n",
    "\n",
    "            for r in removed_records:\n",
    "                r.update({\"folder\": subfolder, \"file\": file_name})\n",
    "            all_removed_records.extend(removed_records)\n",
    "\n",
    "            summary.append({\n",
    "                \"folder\": subfolder,\n",
    "                \"file\": file_name,\n",
    "                \"original_count\": len(explanations),\n",
    "                \"retained_count\": len(filtered),\n",
    "                \"reduction_rate\": 1 - len(filtered) / len(explanations)\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(summary).to_csv(os.path.join(ROOT_FOLDER, \"summary_lexical_n123.csv\"), index=False)\n",
    "    pd.DataFrame(all_removed_records).to_csv(os.path.join(ROOT_FOLDER, \"removed_details_lexical_n123.csv\"), index=False)\n",
    "    print(\"Processed all files and saved summaries.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "147a541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done deduplication\n"
     ]
    }
   ],
   "source": [
    "# 2. pos-tag n-gram\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import shutil\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "ROOT_FOLDER = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\"\n",
    "THRESHOLD = 0.5\n",
    "N_GRAMS = [1, 2, 3]\n",
    "\n",
    "\n",
    "def extract_explanations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub(r\"^\\d+\\.\\s*\", \"\", line.strip()) for line in lines if line.strip()]\n",
    "\n",
    "# def get_ngrams(text, n):\n",
    "#     tokens = text.lower().split()\n",
    "#     return set(zip(*[tokens[i:] for i in range(n)])) if len(tokens) >= n else set()\n",
    "\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    # print(f\"POS tags for '{text}': {pos_tags}\")  # Debugging line to check POS tags\n",
    "    return set(zip(*[pos_tags[i:] for i in range(n)])) if len(pos_tags) >= n else set()\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(a, b, n):\n",
    "    a_ngrams = get_ngrams(a, n)\n",
    "    b_ngrams = get_ngrams(b, n)\n",
    "    union = a_ngrams | b_ngrams\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    intersection = a_ngrams & b_ngrams\n",
    "    diversity = 1 - len(intersection) / len(union)\n",
    "    return diversity\n",
    "\n",
    "def remove_low_lexical_diversity(explanations, n_gram_list, threshold):\n",
    "    kept = []\n",
    "    kept_indices = []\n",
    "    removed_records = []\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        is_duplicate = False\n",
    "        for j, kept_exp in enumerate(kept):\n",
    "            diversities = {\n",
    "                n: compute_lexical_diversity(exp, kept_exp, n) for n in n_gram_list\n",
    "            }\n",
    "            if all(d < threshold for d in diversities.values()):\n",
    "                removed_records.append({\n",
    "                    \"removed_index\": i + 1,\n",
    "                    \"removed_text\": exp,\n",
    "                    \"compared_to_index\": kept_indices[j] + 1,\n",
    "                    \"compared_to_text\": kept_exp,\n",
    "                    **{f\"diversity_{n}gram\": diversities[n] for n in n_gram_list},\n",
    "                    \"is_removed\": True\n",
    "                })\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            kept.append(exp)\n",
    "            kept_indices.append(i)\n",
    "            removed_records.append({\n",
    "                \"removed_index\": i + 1,\n",
    "                \"removed_text\": exp,\n",
    "                \"compared_to_index\": None,\n",
    "                \"compared_to_text\": None,\n",
    "                **{f\"diversity_{n}gram\": None for n in n_gram_list},\n",
    "                \"is_removed\": False\n",
    "            })\n",
    "    return kept, removed_records\n",
    "\n",
    "def process_all():\n",
    "    summary = []\n",
    "    all_removed_records = []\n",
    "\n",
    "    for subfolder in os.listdir(ROOT_FOLDER):\n",
    "        sub_path = os.path.join(ROOT_FOLDER, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "        for file_name in [\"E_first.txt\", \"N_first.txt\", \"C_first.txt\"]:\n",
    "            file_path = os.path.join(sub_path, file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            label_prefix = file_name.split(\"_\")[0]  \n",
    "            output_file = os.path.join(sub_path, f\"{label_prefix}_second.txt\")\n",
    "\n",
    "            explanations = extract_explanations(file_path)\n",
    "            if len(explanations) < 2:\n",
    "                shutil.copyfile(file_path, output_file)\n",
    "                summary.append({\n",
    "                    \"folder\": subfolder,\n",
    "                    \"file\": file_name,\n",
    "                    \"original_count\": len(explanations),\n",
    "                    \"retained_count\": len(explanations),\n",
    "                    \"reduction_rate\": 0.0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            filtered, removed_records = remove_low_lexical_diversity(\n",
    "                explanations, N_GRAMS, THRESHOLD\n",
    "            )\n",
    "\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for idx, line in enumerate(filtered, 1):\n",
    "                    f.write(f\"{idx}. {line}\\n\")\n",
    "\n",
    "            for r in removed_records:\n",
    "                r.update({\"folder\": subfolder, \"file\": file_name})\n",
    "            all_removed_records.extend(removed_records)\n",
    "\n",
    "            summary.append({\n",
    "                \"folder\": subfolder,\n",
    "                \"file\": file_name,\n",
    "                \"original_count\": len(explanations),\n",
    "                \"retained_count\": len(filtered),\n",
    "                \"reduction_rate\": 1 - len(filtered) / len(explanations)\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(summary).to_csv(os.path.join(ROOT_FOLDER, \"summary_postag_n123.csv\"), index=False)\n",
    "    pd.DataFrame(all_removed_records).to_csv(os.path.join(ROOT_FOLDER, \"removed_details_postag_n123.csv\"), index=False)\n",
    "    print(\"Done deduplication\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14b0f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. sentence_embedding\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "SIM_THRESHOLD = 0.9\n",
    "ROOT_FOLDER = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\" \n",
    "\n",
    "def extract_explanations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub(r\"^\\d+\\.\\s*\", \"\", line.strip()) for line in lines if line.strip()]\n",
    "\n",
    "def mean_pairwise_similarity(sim_matrix):\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    return (np.sum(sim_matrix) - np.trace(sim_matrix)) / (n * (n - 1))\n",
    "\n",
    "def remove_similar_explanations(explanations):\n",
    "    embeddings = model.encode(explanations)\n",
    "    sim_matrix = util.cos_sim(embeddings, embeddings).cpu().numpy()\n",
    "    n = len(explanations)\n",
    "    keep_indices = set(range(n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if sim_matrix[i][j] > SIM_THRESHOLD and j in keep_indices:\n",
    "                keep_indices.discard(j)\n",
    "    keep_indices = sorted(keep_indices)\n",
    "    filtered = [explanations[i] for i in keep_indices]\n",
    "    return filtered, embeddings, sim_matrix, keep_indices\n",
    "\n",
    "def plot_pca_original_vs_final(original_explanations, final_explanations, out_path, title=\"PCA: Original vs Final\"):\n",
    "    if len(original_explanations) < 2:\n",
    "        print(f\"Not enough original explanations to plot PCA: {len(original_explanations)}\")\n",
    "        return\n",
    "\n",
    "    all_expl = original_explanations\n",
    "    all_embeddings = model.encode(all_expl)\n",
    "    final_set = set(final_explanations)\n",
    "\n",
    "    mask = [text in final_set for text in all_expl]\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(all_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c='lightgray', alpha=0.4, label=\"Original (All)\")\n",
    "    plt.scatter(reduced[mask, 0], reduced[mask, 1], c='blue', alpha=0.8, label=\"Final (Kept)\")\n",
    "\n",
    "    for i, (x, y) in enumerate(reduced):\n",
    "        plt.text(x + 0.01, y + 0.01, str(i + 1), fontsize=7, color='black')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_similarity_matrix(sim_matrix, out_path, title=\"Similarity Matrix\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True,\n",
    "                xticklabels=np.arange(1, sim_matrix.shape[0] + 1),\n",
    "                yticklabels=np.arange(1, sim_matrix.shape[0] + 1))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def process_all_files(root_folder):\n",
    "    summary = []\n",
    "\n",
    "    for subfolder in os.listdir(root_folder):\n",
    "        sub_path = os.path.join(root_folder, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "        for label in [\"E\", \"N\", \"C\"]:\n",
    "            input_file = os.path.join(sub_path, f\"{label}_second.txt\")\n",
    "            output_file = os.path.join(sub_path, f\"{label}_third.txt\")\n",
    "            if not os.path.isfile(input_file):\n",
    "                continue\n",
    "\n",
    "            explanations = extract_explanations(input_file)\n",
    "            if len(explanations) < 2:\n",
    "                shutil.copyfile(input_file, output_file)\n",
    "                summary.append({\n",
    "                    \"folder\": subfolder,\n",
    "                    \"file\": f\"{label}_second.txt\",\n",
    "                    \"original_count\": len(explanations),\n",
    "                    \"retained_count\": len(explanations),\n",
    "                    \"reduction_rate\": 0.0,\n",
    "                    \"mean_sim_original\": None,\n",
    "                    \"mean_sim_filtered\": None,\n",
    "                    \"similarity_drop\": None,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            filtered, embeddings, sim_matrix, filtered_indices = remove_similar_explanations(explanations)\n",
    "            filtered_embeddings = model.encode(filtered)\n",
    "            filtered_sim_matrix = util.cos_sim(filtered_embeddings, filtered_embeddings).cpu().numpy()\n",
    "\n",
    "            mean_sim_orig = mean_pairwise_similarity(sim_matrix)\n",
    "            mean_sim_filt = mean_pairwise_similarity(filtered_sim_matrix)\n",
    "            similarity_drop = mean_sim_orig - mean_sim_filt\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for idx, line in enumerate(filtered, 1):\n",
    "                    f.write(f\"{idx}. {line}\\n\")\n",
    "\n",
    "            original_file = os.path.join(sub_path, f\"{label}.txt\")\n",
    "            if os.path.isfile(original_file):\n",
    "                original_expl = extract_explanations(original_file)\n",
    "                pca_out = os.path.join(sub_path, f\"{label}_third_pca_from_original.png\")\n",
    "                plot_pca_original_vs_final(original_expl, filtered, pca_out, title=f\"{subfolder}_{label} Original vs Final PCA\")\n",
    "\n",
    "            sim_out = os.path.join(sub_path, f\"{label}_third_sim_matrix.png\")\n",
    "            plot_similarity_matrix(sim_matrix, sim_out, title=f\"{subfolder}_{label}_third Similarity Matrix\")\n",
    "\n",
    "            summary.append({\n",
    "                \"folder\": subfolder,\n",
    "                \"file\": f\"{label}_second.txt\",\n",
    "                \"original_count\": len(explanations),\n",
    "                \"retained_count\": len(filtered),\n",
    "                \"reduction_rate\": 1 - len(filtered) / len(explanations),\n",
    "                \"mean_sim_original\": mean_sim_orig,\n",
    "                \"mean_sim_filtered\": mean_sim_filt,\n",
    "                \"similarity_drop\": similarity_drop,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(summary)\n",
    "    df.to_csv(os.path.join(root_folder, \"summary_sentence_embedding.csv\"), index=False)\n",
    "    print(\"Summary saved to summary.csv\")\n",
    "process_all_files(ROOT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffa9eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary saved to /Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw/explanation_diff_summary_structured.csv\n"
     ]
    }
   ],
   "source": [
    "# collect dedup information\n",
    " \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_FOLDER = \"/Users/phoebeeeee/ongoing/Beyond-noise-MA-Zuo/EACL/qwen_7b_generation_raw\"\n",
    "\n",
    "LABELS = [\"E\", \"N\", \"C\"]\n",
    "STAGE_PATHS = [\n",
    "    (\"0-1\", \"\", \"first\"),\n",
    "    (\"1-2\", \"first\", \"second\"),\n",
    "    (\"2-3\", \"second\", \"third\"),\n",
    "]\n",
    "\n",
    "def extract_indexed_explanations(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            match = re.match(r\"^(\\d+)\\.\\s*(.*)\", line)\n",
    "            if match:\n",
    "                idx = int(match.group(1))\n",
    "                content = match.group(2)\n",
    "                lines.append((idx, content))\n",
    "    return lines\n",
    "\n",
    "def compare_files(from_list, to_list):\n",
    "    from_dict = dict(from_list)\n",
    "    to_indices = set(idx for idx, _ in to_list)\n",
    "    dropped_indices = sorted(set(from_dict.keys()) - to_indices)\n",
    "    dropped_texts = [from_dict[idx] for idx in dropped_indices]\n",
    "    return dropped_indices, dropped_texts\n",
    "\n",
    "def compare_all_stages():\n",
    "    all_records = []\n",
    "\n",
    "    for subfolder in os.listdir(ROOT_FOLDER):\n",
    "        sub_path = os.path.join(ROOT_FOLDER, subfolder)\n",
    "        if not os.path.isdir(sub_path):\n",
    "            continue\n",
    "\n",
    "        for label in LABELS:\n",
    "            row = {\n",
    "                \"folder\": subfolder,\n",
    "                \"label\": label,\n",
    "                \"0-1\": {\"num\": 0, \"text\": []},\n",
    "                \"1-2\": {\"num\": 0, \"text\": []},\n",
    "                \"2-3\": {\"num\": 0, \"text\": []},\n",
    "                \"dropped_count\": 0,\n",
    "                # \"dropped_indices\": [],\n",
    "            }\n",
    "\n",
    "            all_dropped_indices = set()\n",
    "\n",
    "            for stage_name, from_tag, to_tag in STAGE_PATHS:\n",
    "                from_file = f\"{label}.txt\" if from_tag == \"\" else f\"{label}_{from_tag}.txt\"\n",
    "                to_file = f\"{label}_{to_tag}.txt\"\n",
    "\n",
    "                from_path = os.path.join(sub_path, from_file)\n",
    "                to_path = os.path.join(sub_path, to_file)\n",
    "\n",
    "                if not os.path.isfile(from_path) or not os.path.isfile(to_path):\n",
    "                    continue\n",
    "\n",
    "                from_data = extract_indexed_explanations(from_path)\n",
    "                to_data = extract_indexed_explanations(to_path)\n",
    "\n",
    "                dropped_indices, dropped_texts = compare_files(from_data, to_data)\n",
    "\n",
    "                row[stage_name][\"num\"] = len(dropped_indices)\n",
    "                row[stage_name][\"text\"] = dropped_texts\n",
    "                all_dropped_indices.update(dropped_indices)\n",
    "\n",
    "            row[\"dropped_count\"] = len(all_dropped_indices)\n",
    "            # row[\"dropped_indices\"] = sorted(all_dropped_indices)\n",
    "\n",
    "            all_records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    output_csv = os.path.join(ROOT_FOLDER, \"explanation_diff_summary_structured.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Summary saved to {output_csv}\")\n",
    "\n",
    "compare_all_stages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1825f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
